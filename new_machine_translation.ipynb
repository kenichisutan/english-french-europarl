{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation: English-French Europarl\n",
    "\n",
    "This notebook implements three translation approaches:\n",
    "1. **Baseline**: Word-for-word translation using a bilingual dictionary\n",
    "2. **Advanced**: Cross-lingual embeddings for semantic translation\n",
    "3. **Best Model**: Fine-tuned Seq2Seq Transformer (iterative improvement)\n",
    "\n",
    "We'll evaluate all models on a train/test split of the Europarl corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_libraries = [\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",         \n",
    "    \"nltk\",\n",
    "    \"sentence-transformers\",\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"accelerate>=0.26.0\",   \n",
    "    \"sentencepiece\",\n",
    "    \"packaging\",\n",
    "    \"torch\"\n",
    "]\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + required_libraries)\n",
    "    print(\"All libraries installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Installation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found English file: Europarl.en-fr.en\n",
      "Found French file:  Europarl.en-fr.fr\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Setup Paths\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "DATA_DIR = NOTEBOOK_DIR / \"data\"\n",
    "\n",
    "# Define the specific files you have\n",
    "EN_PATH = DATA_DIR / \"Europarl.en-fr.en\"\n",
    "FR_PATH = DATA_DIR / \"Europarl.en-fr.fr\"\n",
    "\n",
    "# Verify they exist\n",
    "if EN_PATH.exists() and FR_PATH.exists():\n",
    "    print(f\"Found English file: {EN_PATH.name}\")\n",
    "    print(f\"Found French file:  {FR_PATH.name}\")\n",
    "else:\n",
    "    print(\"Error: Files not found. Check your folder structure.\")\n",
    "\n",
    "def load_parallel_subset(en_path, fr_path, max_lines):\n",
    "    \"\"\"\n",
    "    Reads two aligned text files and returns a Pandas DataFrame with columns ['en', 'fr'].\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    try:\n",
    "        # Open both files at the same time\n",
    "        with open(en_path, \"r\", encoding=\"utf-8\") as f_en, \\\n",
    "             open(fr_path, \"r\", encoding=\"utf-8\") as f_fr:\n",
    "            \n",
    "            # zip() pairs lines together safely\n",
    "            for i, (line_en, line_fr) in enumerate(zip(f_en, f_fr)):\n",
    "                if i >= max_lines:\n",
    "                    break\n",
    "                \n",
    "                text_en = line_en.strip()\n",
    "                text_fr = line_fr.strip()\n",
    "                \n",
    "                # Only keep if both sides have content\n",
    "                if text_en and text_fr:\n",
    "                    pairs.append((text_en, text_fr))\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DF on error\n",
    "\n",
    "    # Convert list of tuples to DataFrame\n",
    "    df = pd.DataFrame(pairs, columns=[\"en\", \"fr\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 100000 pairs.\n"
     ]
    }
   ],
   "source": [
    "subset_size = 100000\n",
    "\n",
    "# Run the loader\n",
    "df = load_parallel_subset(EN_PATH, FR_PATH, max_lines=subset_size)\n",
    "print(f\"Successfully loaded {len(df)} pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (100000, 2)\n",
      "Pairs list length: 100000\n"
     ]
    }
   ],
   "source": [
    "# Run the new loader\n",
    "df = load_parallel_subset(EN_PATH, FR_PATH, max_lines=subset_size)\n",
    "\n",
    "# --- THE FIX ---\n",
    "# Convert back to list of tuples for your old analysis functions\n",
    "pairs = df.to_records(index=False).tolist()\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Pairs list length: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Moreover, a concentration of 10 g/m3 must not be exceeded during that period.',\n",
       "  'En outre, durant cette période, les intéressés ne pourront pas dépasser une concentration de 10 g/m3.'),\n",
       " ('I should point out that the proposal for Serbia for the year 2001 makes provision for a fixed sum of 40 million.',\n",
       "  \"Je peux signaler que, pour 2001, la proposition relative à la Serbie prévoit un chiffre ferme de 40 millions d'euros.\"),\n",
       " ('Of course, I am not saying that there is too much money floating around; as Commissioner for the budget I am aware that money is always scarce, but money is also scarce for those who have to finance the European budget.',\n",
       "  \"Je ne dis naturellement pas qu'il y a trop d'argent dans certains domaines mais, en tant que commissaire en charge du budget, j'ai conscience que, si les ressources sont toujours limitées, elles le sont aussi pour ceux qui doivent financer le budget européen.\"),\n",
       " ('Human rights do not exist in isolation, they are connected to our development policies, our economic policies, our home affairs policies, indeed they are at the very heart of our democratic functions.',\n",
       "  \"Les droits de l'homme n'existent pas isolément, ils sont liés à nos politiques de développement, à nos politiques économiques, à nos politiques des affaires intérieures ; ils sont en fait au cur même de nos fonctions démocratiques.\"),\n",
       " ('In the document that was approved by Parliament we called for several amendments, among others, that the minimum age for taking fingerprints be set at 18.',\n",
       "  \"Dans le document adopté par le Parlement, nous avons demandé une série de modifications, entre autres celle de fixer l'âge des personnes dont on prendrait les empreintes digitales à dix-huit ans.\"),\n",
       " ('It will lead to unnecessary and laborious inspections of Russian wagons and to massive bureaucracy.',\n",
       "  'Elle aurait pour conséquence un examen inutile et laborieux des wagons russes et mènerait à une gigantesque bureaucratie.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets\n",
    "train_pairs, test_pairs = train_test_split(\n",
    "    pairs, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "train_pairs[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: Word-for-Word Translation\n",
    "\n",
    "This baseline model builds a bilingual dictionary from the training data and translates each word independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bilingual dictionary...\n",
      "Dictionary built with 25,178 English words\n"
     ]
    }
   ],
   "source": [
    "class WordForWordTranslator:\n",
    "    \"\"\"Baseline word-for-word translation using bilingual dictionary.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_dict = defaultdict(lambda: defaultdict(int))\n",
    "        self.most_common_translations = {}\n",
    "        \n",
    "    def train(self, train_pairs):\n",
    "        \"\"\"Build bilingual dictionary from training pairs.\"\"\"\n",
    "        print(\"Building bilingual dictionary...\")\n",
    "        \n",
    "        for en_text, fr_text in train_pairs:\n",
    "            # Simple tokenization (split on whitespace and punctuation)\n",
    "            en_words = re.findall(r'\\b\\w+\\b', en_text.lower())\n",
    "            fr_words = re.findall(r'\\b\\w+\\b', fr_text.lower())\n",
    "            \n",
    "            if not en_words or not fr_words:\n",
    "                continue\n",
    "            \n",
    "            # Use positional alignment based on relative position\n",
    "            # This prevents common words like \"de\" from dominating\n",
    "            en_len = len(en_words)\n",
    "            fr_len = len(fr_words)\n",
    "            \n",
    "            # Align words based on their relative positions\n",
    "            for i, en_word in enumerate(en_words):\n",
    "                # Map English position to French position\n",
    "                fr_pos = int((i / en_len) * fr_len)\n",
    "                fr_pos = min(fr_pos, fr_len - 1)  # Ensure valid index\n",
    "                \n",
    "                # Align to the word at the corresponding position\n",
    "                fr_word = fr_words[fr_pos]\n",
    "                self.word_dict[en_word][fr_word] += 1\n",
    "                \n",
    "                # Also align to nearby words (within 1 position) for better coverage\n",
    "                if fr_pos > 0:\n",
    "                    self.word_dict[en_word][fr_words[fr_pos - 1]] += 0.5\n",
    "                if fr_pos < fr_len - 1:\n",
    "                    self.word_dict[en_word][fr_words[fr_pos + 1]] += 0.5\n",
    "        \n",
    "        # Store most common translation for each English word\n",
    "        for en_word, fr_translations in self.word_dict.items():\n",
    "            if fr_translations:\n",
    "                self.most_common_translations[en_word] = max(\n",
    "                    fr_translations.items(), \n",
    "                    key=lambda x: x[1]\n",
    "                )[0]\n",
    "        \n",
    "        print(f\"Dictionary built with {len(self.most_common_translations):,} English words\")\n",
    "        \n",
    "    def translate(self, en_text):\n",
    "        \"\"\"Translate English text word-by-word.\"\"\"\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en_text)\n",
    "        fr_words = []\n",
    "        \n",
    "        for word in en_words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in self.most_common_translations:\n",
    "                fr_words.append(self.most_common_translations[word_lower])\n",
    "            else:\n",
    "                # Unknown word - keep original\n",
    "                fr_words.append(word)\n",
    "        \n",
    "        return ' '.join(fr_words)\n",
    "    \n",
    "    def translate_preserve_case(self, en_text):\n",
    "        \"\"\"Translate preserving original word casing.\"\"\"\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en_text)\n",
    "        fr_words = []\n",
    "        \n",
    "        for word in en_words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in self.most_common_translations:\n",
    "                translation = self.most_common_translations[word_lower]\n",
    "                # Preserve case\n",
    "                if word[0].isupper():\n",
    "                    translation = translation.capitalize()\n",
    "                fr_words.append(translation)\n",
    "            else:\n",
    "                fr_words.append(word)\n",
    "        \n",
    "        return ' '.join(fr_words)\n",
    "\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = WordForWordTranslator()\n",
    "baseline_model.train(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Translation Examples:\n",
      "\n",
      "Example 1:\n",
      "  EN: You see, if we want to train professionals effectively, it is essential to teach them to understand the European and international dimensions of the market in their fields - and to do so at European level too.\n",
      "  FR (true):  Voyez-vous, si nous voulons former des professionnels dans une optique d'efficacité, il est essentiel, y compris au niveau européen, de les former à appréhender les dimensions européenne et internationale du marché dans leurs domaines d'activité.\n",
      "  FR (pred):  Vous que si nous nous de train professionnels de il est est de à les de je la L et la dimensions de la marché dans de domaines et de ne de à L niveau trop\n",
      "\n",
      "Example 2:\n",
      "  EN: This means that their specific importance in many cases, given the significant quantities they have to distribute, is greater than that of the ambassadors and of the Member States.\n",
      "  FR (true):  Cela signifie que leur poids spécifique est dans de nombreux cas, compte tenu des quantités importantes qu'ils doivent distribuer, supérieur au rôle des ambassadeurs et des États membres.\n",
      "  FR (pred):  Ce de que de des importance dans de cas de la de quantités ils nous de de est plus que que de la ambassadeurs et de la États États\n",
      "\n",
      "Example 3:\n",
      "  EN: There are two political reasons why our committee and myself as its chairman must speak out about this issue.\n",
      "  FR (true):  Une double considération politique conduit notre commission - ainsi que moi-même, en tant que Président de cette commission - à élever la voix dans cette affaire.\n",
      "  FR (pred):  Il nous deux politique raisons pourquoi notre commission et moi comme de président doit de de de ce question\n",
      "\n",
      "Example 4:\n",
      "  EN: I well remember its genesis and the problems that were discussed at the time.\n",
      "  FR (true):  Je me rappelle encore très bien son élaboration et les problèmes dont nous avons autrefois discuté.\n",
      "  FR (pred):  Je bien rappeler de 925 et la problèmes que de de à la temps\n",
      "\n",
      "Example 5:\n",
      "  EN: Apart from the fact that we cannot go into the issue in depth in an urgent debate, this accident was too serious, 80 people lost their lives, and I think that the European Parliament in particular is obliged to take an immediate position.\n",
      "  FR (true):  Même si un débat au titre de l' urgence ne peut aller au fond des choses, il s' agit d' un accident très grave où 80 personnes ont perdu la vie, et je pense qu' il est tout à fait impératif que le Parlement européen prenne immédiatement position.\n",
      "  FR (pred):  En de la que que nous ne de en la question dans et dans un d débat ce accident a trop de 80 de perdu de vie et Je je que la L Parlement dans en est de de de un de position\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the baseline model on a few examples\n",
    "print(\"Baseline Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = baseline_model.translate_preserve_case(en)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Model: Cross-Lingual Embeddings\n",
    "\n",
    "This model uses pre-trained multilingual embeddings to find the best translation by comparing semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'PreTrainedModel'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2044\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2046\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2239\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2237\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:86\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     78\u001b[39m     ALL_PARALLEL_STYLES,\n\u001b[32m     79\u001b[39m     _get_parameter_tp_plan,\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m     verify_tp_plan,\n\u001b[32m     85\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_flash_attention_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lazy_import_flash_attention, lazy_import_paged_flash_attention\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_d_fine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DFineForObjectDetectionLoss\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_deformable_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\loss\\loss_d_fine.py:21\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_vision_available\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_for_object_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_iou\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloss_rt_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDetrHungarianMatcher, RTDetrLoss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\loss\\loss_for_object_detection.py:31\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdice_loss\u001b[39m(inputs, targets, num_boxes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\image_transforms.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     ChannelDimension,\n\u001b[32m     24\u001b[39m     ImageInput,\n\u001b[32m     25\u001b[39m     get_channel_dimension_axis,\n\u001b[32m     26\u001b[39m     get_image_size,\n\u001b[32m     27\u001b[39m     infer_channel_dimension_format,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_torch_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\image_utils.py:53\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[32m     55\u001b[39m     pil_torch_interpolation_mapping = {\n\u001b[32m     56\u001b[39m         PILImageResampling.NEAREST: InterpolationMode.NEAREST_EXACT,\n\u001b[32m     57\u001b[39m         PILImageResampling.BOX: InterpolationMode.BOX,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m         PILImageResampling.LANCZOS: InterpolationMode.LANCZOS,\n\u001b[32m     62\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\_meta_registrations.py:163\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad.new_empty((batch_size, channels, height, width))\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorchvision::nms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\library.py:1073\u001b[39m, in \u001b[36mregister_fake.<locals>.register\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m   1072\u001b[39m     use_lib = lib\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m \u001b[43muse_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_override\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\library.py:203\u001b[39m, in \u001b[36mLibrary._register_fake\u001b[39m\u001b[34m(self, op_name, fn, _stacklevel, allow_override)\u001b[39m\n\u001b[32m    201\u001b[39m     func_to_register = fn\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m handle = \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_to_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_override\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m._registration_handles.append(handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_library\\fake_impl.py:50\u001b[39m, in \u001b[36mFakeImplHolder.register\u001b[39m\u001b[34m(self, func, source, lib, allow_override)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     46\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.qualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33malready has an fake impl registered at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.kernel.source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     52\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.qualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: operator torchvision::nms does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCrossLingualEmbeddingTranslator\u001b[39;00m:\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Translation using cross-lingual embeddings and semantic similarity.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\__init__.py:15\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     AutoConfig,\n\u001b[32m     23\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m     24\u001b[39m     AutoTokenizer,\n\u001b[32m     25\u001b[39m     PretrainedConfig,\n\u001b[32m     26\u001b[39m     PreTrainedModel,\n\u001b[32m     27\u001b[39m     PreTrainedTokenizer,\n\u001b[32m     28\u001b[39m     is_torch_npu_available,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2133\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2129\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2130\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2131\u001b[39m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2132\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2133\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2134\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2135\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2137\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   2138\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Could not import module 'PreTrainedModel'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CrossLingualEmbeddingTranslator:\n",
    "    \"\"\"Translation using cross-lingual embeddings and semantic similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with a multilingual sentence transformer.\n",
    "        Options:\n",
    "        - 'paraphrase-multilingual-MiniLM-L12-v2' (fast, good quality)\n",
    "        - 'paraphrase-multilingual-mpnet-base-v2' (slower, better quality)\n",
    "        - 'distiluse-base-multilingual-cased' (alternative)\n",
    "        \"\"\"\n",
    "        print(f\"Loading multilingual embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.french_candidates = []\n",
    "        self.french_embeddings = None\n",
    "        \n",
    "    def train(self, train_pairs):\n",
    "        \"\"\"Build a candidate pool of French translations from training data.\"\"\"\n",
    "        print(\"Building French candidate pool...\")\n",
    "        \n",
    "        # Collect unique French sentences (or a large sample)\n",
    "        french_sentences = set()\n",
    "        for _, fr_text in train_pairs:\n",
    "            if fr_text.strip():\n",
    "                french_sentences.add(fr_text.strip())\n",
    "        \n",
    "        # Limit to reasonable size for efficiency (can be adjusted)\n",
    "        max_candidates = 50000\n",
    "        if len(french_sentences) > max_candidates:\n",
    "            french_sentences = list(french_sentences)[:max_candidates]\n",
    "        else:\n",
    "            french_sentences = list(french_sentences)\n",
    "        \n",
    "        self.french_candidates = french_sentences\n",
    "        \n",
    "        # Pre-compute embeddings for all French candidates\n",
    "        print(f\"Computing embeddings for {len(self.french_candidates):,} French candidates...\")\n",
    "        self.french_embeddings = self.model.encode(\n",
    "            self.french_candidates,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "    def translate(self, en_text, top_k=1):\n",
    "        \"\"\"\n",
    "        Translate by finding the most semantically similar French sentence.\n",
    "        \n",
    "        Args:\n",
    "            en_text: English text to translate\n",
    "            top_k: Number of top candidates to return (default: 1, returns best match)\n",
    "        \"\"\"\n",
    "        if not en_text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Get embedding for English text\n",
    "        en_embedding = self.model.encode([en_text])\n",
    "        \n",
    "        # Compute cosine similarity with all French candidates\n",
    "        similarities = cosine_similarity(en_embedding, self.french_embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        if top_k == 1:\n",
    "            return self.french_candidates[top_indices[0]]\n",
    "        else:\n",
    "            return [self.french_candidates[idx] for idx in top_indices]\n",
    "\n",
    "\n",
    "# Train the cross-lingual embedding model\n",
    "print(\"Training cross-lingual embedding model...\")\n",
    "embedding_model = CrossLingualEmbeddingTranslator()\n",
    "embedding_model.train(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_semantics(model, words_en, words_fr):\n",
    "    \"\"\"\n",
    "    Visualizes English and French word pairs in 2D space using PCA.\n",
    "    \"\"\"\n",
    "    # 1. Compute embeddings\n",
    "    embeddings_en = model.model.encode(words_en)\n",
    "    embeddings_fr = model.model.encode(words_fr)\n",
    "    \n",
    "    # 2. Combine for PCA\n",
    "    all_embeddings = np.vstack([embeddings_en, embeddings_fr])\n",
    "    \n",
    "    # 3. Reduce to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    results = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot English words (Blue)\n",
    "    plt.scatter(results[:len(words_en), 0], results[:len(words_en), 1], c='blue', label='English')\n",
    "    for i, word in enumerate(words_en):\n",
    "        plt.annotate(word, xy=(results[i, 0], results[i, 1]), xytext=(5, 2), \n",
    "                     textcoords='offset points', color='blue')\n",
    "                     \n",
    "    # Plot French words (Red)\n",
    "    plt.scatter(results[len(words_en):, 0], results[len(words_en):, 1], c='red', label='French')\n",
    "    for i, word in enumerate(words_fr):\n",
    "        plt.annotate(word, xy=(results[len(words_en)+i, 0], results[len(words_en)+i, 1]), \n",
    "                     xytext=(5, 2), textcoords='offset points', color='red')\n",
    "    \n",
    "    # Draw lines between pairs to show alignment\n",
    "    for i in range(len(words_en)):\n",
    "        plt.plot([results[i, 0], results[len(words_en)+i, 0]],\n",
    "                 [results[i, 1], results[len(words_en)+i, 1]], \n",
    "                 'k--', alpha=0.2)\n",
    "\n",
    "    plt.title('Cross-Lingual Semantic Space (PCA)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Define pairs to visualize (conceptually similar words should be close)\n",
    "concepts = [\n",
    "    (\"democracy\", \"démocratie\"),\n",
    "    (\"parliament\", \"parlement\"),\n",
    "    (\"law\", \"loi\"),\n",
    "    (\"commission\", \"commission\"),\n",
    "    (\"vote\", \"vote\"),\n",
    "    (\"president\", \"président\"),\n",
    "    (\"money\", \"argent\"),\n",
    "    (\"crisis\", \"crise\")\n",
    "]\n",
    "\n",
    "words_en = [c[0] for c in concepts]\n",
    "words_fr = [c[1] for c in concepts]\n",
    "\n",
    "visualize_semantics(embedding_model, words_en, words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Lingual Embedding Translation Examples:\n",
      "\n",
      "Example 1:\n",
      "  EN: The implications are significant for the agro-food sector overall, which has been struck squarely by this crisis.\n",
      "  FR (true):  Les conséquences sont importantes pour l'ensemble de ce secteur agro-alimentaire qui a été touché de plein fouet par cette crise.\n",
      "  FR (pred):  Ce sont justement les bénéfices qui sont au départ des crises actuelles au niveau de la sécurité alimentaire.\n",
      "\n",
      "Example 2:\n",
      "  EN: The EUR 614 million adopted by the Council will be enough to finance all the foreseeable needs.\n",
      "  FR (true):  Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "  FR (pred):  Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "\n",
      "Example 3:\n",
      "  EN: Joint motion for a resolution (B5-0181/2000) on the shipwreck of the .\n",
      "  FR (true):  Proposition de résolution commune (B5-0181/2000) sur le naufrage de l'Erika.\n",
      "  FR (pred):  Résolution commune sur le naufrage de l'Erika (RC B5-0181/2000)\n",
      "\n",
      "Example 4:\n",
      "  EN: Following the debates with the Council last night I was wondering whether this report is actually close enough to the original proposal for first reading procedure to be possible.\n",
      "  FR (true):  À la suite aux débats du Conseil hier soir, je me demandais si ce rapport était en fait assez proche de la proposition originale pour permettre la procédure de première lecture.\n",
      "  FR (pred):  Lorsque j'ai rédigé ce rapport, j'y ai initialement introduit quelques idées polémiques afin de vérifier si on avait pris la peine de le lire.\n",
      "\n",
      "Example 5:\n",
      "  EN: Mr Prodi has spoken in favour of preserving the Community model, which met with great approval throughout this Chamber, and I would like, in my turn, to welcome this speech which will doubtless continue to be viewed as a major one.\n",
      "  FR (true):  M. Prodi, président de la Commission, a fait un plaidoyer pour la préservation du modèle communautaire, qui a recueilli un très fort assentiment sur tous ces bancs et je veux, à mon tour, saluer ce discours qui restera, sans nul doute, un discours important.\n",
      "  FR (pred):  Je conclurai au nom de mon groupe en disant que M. Prodi a exposé aujourd'hui ce en quoi nous croyons.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the embedding model on a few examples\n",
    "print(\"Cross-Lingual Embedding Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = embedding_model.translate(en)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Model: Fine-tuned Seq2Seq Transformer\n",
    "\n",
    "This is our iterative improvement model. We'll fine-tune a pretrained transformer model on our Europarl data to achieve the best translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTranslator:\n",
    "    \"\"\"Fine-tuned Seq2Seq Transformer for English-French translation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='Helsinki-NLP/opus-mt-en-fr', max_length=128):\n",
    "        \"\"\"\n",
    "        Initialize with a pretrained translation model.\n",
    "        \n",
    "        Options:\n",
    "        - 'Helsinki-NLP/opus-mt-en-fr' (fast, good baseline, ~300MB)\n",
    "        - 'facebook/mbart-large-50' (multilingual, better quality, ~2.5GB, needs more GPU)\n",
    "        - 'google/mt5-base' (multilingual T5, good quality, ~850MB)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure sentencepiece is installed (required for opus-mt and some other models)\n",
    "        try:\n",
    "            import sentencepiece\n",
    "        except ImportError:\n",
    "            print(\"Installing sentencepiece (required for this tokenizer)...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sentencepiece'])\n",
    "            import sentencepiece\n",
    "            print(\"sentencepiece installed successfully!\")\n",
    "        \n",
    "        print(f\"Initializing model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "        \n",
    "    def train(self, train_pairs, val_pairs=None, \n",
    "              num_epochs=3, batch_size=8, learning_rate=5e-5,\n",
    "              save_steps=1000, eval_steps=500, warmup_steps=500):\n",
    "        \"\"\"\n",
    "        Fine-tune the model on training data.\n",
    "        \n",
    "        Args:\n",
    "            train_pairs: List of (en, fr) tuples for training\n",
    "            val_pairs: Optional validation set (if None, uses 10% of train)\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Training batch size (adjust based on GPU memory)\n",
    "            learning_rate: Learning rate for fine-tuning\n",
    "            save_steps: Save checkpoint every N steps\n",
    "            eval_steps: Evaluate every N steps\n",
    "            warmup_steps: Warmup steps for learning rate scheduler\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Training Seq2Seq Transformer Model\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Training examples: {len(train_pairs):,}\")\n",
    "\n",
    "        import json \n",
    "        import os  \n",
    "\n",
    "        # Prepare validation set\n",
    "        if val_pairs is None:\n",
    "            # Use 10% of training data for validation\n",
    "            val_size = max(1000, len(train_pairs) // 10)\n",
    "            val_pairs = train_pairs[:val_size]\n",
    "            train_pairs = train_pairs[val_size:]\n",
    "            print(f\"Split: {len(train_pairs):,} train, {len(val_pairs):,} validation\")\n",
    "        \n",
    "        # Convert to HuggingFace Dataset format\n",
    "        def prepare_dataset(pairs):\n",
    "            return Dataset.from_dict({\n",
    "                'en': [pair[0] for pair in pairs],\n",
    "                'fr': [pair[1] for pair in pairs]\n",
    "            })\n",
    "        \n",
    "        train_dataset = prepare_dataset(train_pairs)\n",
    "        val_dataset = prepare_dataset(val_pairs)\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        def tokenize_function(examples):\n",
    "            # Tokenize English (source) and French (target)\n",
    "            model_inputs = self.tokenizer(\n",
    "                examples['en'],\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            \n",
    "            # Tokenize French targets\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                labels = self.tokenizer(\n",
    "                    examples['fr'],\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding='max_length'\n",
    "                )\n",
    "            \n",
    "            # Replace padding token id's of the labels with -100 (ignored by loss)\n",
    "            labels['input_ids'] = [\n",
    "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels['input_ids']\n",
    "            ]\n",
    "            \n",
    "            model_inputs['labels'] = labels['input_ids']\n",
    "            return model_inputs\n",
    "        \n",
    "        print(\"Tokenizing datasets...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        val_dataset = val_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        output_dir = f\"./seq2seq_model_{self.model_name.split('/')[-1]}\"\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f'{output_dir}/logs',\n",
    "            logging_steps=100,\n",
    "            eval_steps=eval_steps,\n",
    "            save_steps=save_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            fp16=torch.cuda.is_available(),  # Use mixed precision if CUDA GPU available (MPS doesn't support fp16)\n",
    "            report_to=\"none\",  # Disable wandb/tensorboard\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Train!\n",
    "        print(\"\\nStarting training...\")\n",
    "        print(f\"Epochs: {num_epochs}, Batch size: {batch_size}, Learning rate: {learning_rate}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: CUDA available\")\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            print(f\"GPU: Apple Silicon (MPS) available\")\n",
    "        else:\n",
    "            print(f\"Using: CPU\")\n",
    "        \n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "        log_history_path = os.path.join(output_dir, \"training_logs.json\")\n",
    "        with open(log_history_path, \"w\") as f:\n",
    "            json.dump(trainer.state.log_history, f)\n",
    "        print(f\"Training logs saved to {log_history_path}\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def translate(self, en_text, max_length=None, num_beams=4, do_sample=False):\n",
    "        \"\"\"\n",
    "        Translate English text to French.\n",
    "        \n",
    "        Args:\n",
    "            en_text: English text to translate\n",
    "            max_length: Maximum output length (default: self.max_length)\n",
    "            num_beams: Number of beams for beam search (higher = better quality, slower)\n",
    "            do_sample: Whether to use sampling (False = deterministic)\n",
    "        \"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            en_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=do_sample,\n",
    "                early_stopping=True,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation\n",
    "    \n",
    "    def translate_batch(self, en_texts, batch_size=8, **kwargs):\n",
    "        \"\"\"Translate a batch of English texts.\"\"\"\n",
    "        translations = []\n",
    "        for i in range(0, len(en_texts), batch_size):\n",
    "            batch = en_texts[i:i+batch_size]\n",
    "            batch_translations = [self.translate(text, **kwargs) for text in batch]\n",
    "            translations.extend(batch_translations)\n",
    "        return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: Helsinki-NLP/opus-mt-en-fr...\n",
      "Model loaded on mps\n",
      "\n",
      "Model initialized and ready for training!\n",
      "Note: Training will take time. Adjust batch_size and num_epochs based on your GPU memory.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the seq2seq model\n",
    "# You can adjust the model_name to try different pretrained models\n",
    "# For faster training/testing, start with opus-mt-en-fr\n",
    "# For better quality (if you have GPU memory), try mBART or mT5\n",
    "\n",
    "seq2seq_model = Seq2SeqTranslator(\n",
    "    model_name='Helsinki-NLP/opus-mt-en-fr',  # Good baseline, fast\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(\"\\nModel initialized and ready for training!\")\n",
    "print(\"Note: Training will take time. Adjust batch_size and num_epochs based on your GPU memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "**Adjust these parameters based on your resources:**\n",
    "\n",
    "- **CUDA GPU (NVIDIA)**: Use larger batch_size (16-32), more epochs (3-5), fp16 enabled\n",
    "- **Apple Silicon GPU (M1/M2/M3)**: Use moderate batch_size (8-16), more epochs (3-5), no fp16\n",
    "- **CPU only**: Use smaller batch_size (2-4), fewer epochs (1-2), expect slower training\n",
    "- **Limited GPU memory**: Reduce batch_size, use gradient accumulation\n",
    "\n",
    "**Model options:**\n",
    "- `Helsinki-NLP/opus-mt-en-fr`: Fast, ~300MB, good for quick iteration\n",
    "- `facebook/mbart-large-50`: Better quality, ~2.5GB, needs more GPU memory\n",
    "- `google/mt5-base`: Good balance, ~850MB\n",
    "\n",
    "**Note for Apple Silicon (MacBook):**\n",
    "- The code automatically detects and uses MPS (Metal Performance Shaders)\n",
    "- Training will be faster than CPU but may be slower than high-end NVIDIA GPUs\n",
    "- Some operations may fall back to CPU if not supported by MPS\n",
    "- Mixed precision (fp16) is not supported on MPS, so training uses full precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Seq2Seq Transformer Model\n",
      "============================================================\n",
      "Training examples: 80,000\n",
      "Split: 72,000 train, 8,000 validation\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23bc0632dce42c58c52df6d5c65bb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1748454241d443f49ec4bc60c4b72158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epochs: 3, Batch size: 8, Learning rate: 5e-05\n",
      "GPU: Apple Silicon (MPS) available\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27000' max='27000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27000/27000 2:56:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.218300</td>\n",
       "      <td>1.173248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.285200</td>\n",
       "      <td>1.190946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.257800</td>\n",
       "      <td>1.188723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.308700</td>\n",
       "      <td>1.184073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.266800</td>\n",
       "      <td>1.183677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.295100</td>\n",
       "      <td>1.182387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.244700</td>\n",
       "      <td>1.180292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.245600</td>\n",
       "      <td>1.178450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.259900</td>\n",
       "      <td>1.176777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.245000</td>\n",
       "      <td>1.172938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.269900</td>\n",
       "      <td>1.168889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.231200</td>\n",
       "      <td>1.165986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.221800</td>\n",
       "      <td>1.163944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>1.162602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.204800</td>\n",
       "      <td>1.161006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.214600</td>\n",
       "      <td>1.154776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.228300</td>\n",
       "      <td>1.154295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.188800</td>\n",
       "      <td>1.151486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>1.161055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.029200</td>\n",
       "      <td>1.157775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.026700</td>\n",
       "      <td>1.159834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.030600</td>\n",
       "      <td>1.154015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>1.157537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.056800</td>\n",
       "      <td>1.154649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.059000</td>\n",
       "      <td>1.153106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.063000</td>\n",
       "      <td>1.151992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.065300</td>\n",
       "      <td>1.149729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>1.152294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.074200</td>\n",
       "      <td>1.147496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.061500</td>\n",
       "      <td>1.147137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.035600</td>\n",
       "      <td>1.145854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.049100</td>\n",
       "      <td>1.143907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.053800</td>\n",
       "      <td>1.143697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.036400</td>\n",
       "      <td>1.140928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.045300</td>\n",
       "      <td>1.139670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.042600</td>\n",
       "      <td>1.138802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.909800</td>\n",
       "      <td>1.148104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.947300</td>\n",
       "      <td>1.144742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>1.145998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.916500</td>\n",
       "      <td>1.146490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>1.145870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>1.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>1.144098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>1.144819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>1.142187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>1.141725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.897600</td>\n",
       "      <td>1.141006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>1.140615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>1.139028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>1.137742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>1.137122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>1.136064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.914200</td>\n",
       "      <td>1.136246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>1.135735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 1.0713\n",
      "Model saved to ./seq2seq_model_opus-mt-en-fr\n",
      "\n",
      "✅ Training complete! Model is ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Train the seq2seq model\n",
    "# Adjust parameters based on your GPU/CPU and time constraints\n",
    "\n",
    "# For quick testing (CPU or limited time):\n",
    "# seq2seq_model.train(\n",
    "#     train_pairs,\n",
    "#     num_epochs=1,\n",
    "#     batch_size=4 if not torch.cuda.is_available() else 8,\n",
    "#     learning_rate=5e-5,\n",
    "#     save_steps=500,\n",
    "#     eval_steps=250\n",
    "# )\n",
    "\n",
    "# For better results (with GPU):\n",
    "# Determine batch size based on available device\n",
    "if torch.cuda.is_available():\n",
    "    batch_size = 16  # CUDA GPU - can use larger batches\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    batch_size = 8   # Apple Silicon GPU - moderate batch size\n",
    "else:\n",
    "    batch_size = 4   # CPU - smaller batches\n",
    "\n",
    "trainer = seq2seq_model.train(\n",
    "    train_pairs,\n",
    "    num_epochs=3,  # Increase to 5-10 for better results\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=5e-5,  # Try 3e-5 or 1e-4 for experimentation\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    warmup_steps=500\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete! Model is ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss from the HuggingFace Trainer history.\n",
    "    \"\"\"\n",
    "    # Retrieve logs\n",
    "    history = trainer.state.log_history\n",
    "    \n",
    "    # Extract loss values\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_steps = []\n",
    "    val_steps = []\n",
    "    \n",
    "    for entry in history:\n",
    "        if 'loss' in entry:\n",
    "            train_loss.append(entry['loss'])\n",
    "            train_steps.append(entry['step'])\n",
    "        if 'eval_loss' in entry:\n",
    "            val_loss.append(entry['eval_loss'])\n",
    "            val_steps.append(entry['step'])\n",
    "            \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_steps, train_loss, label='Training Loss', color='blue', alpha=0.6)\n",
    "    plt.plot(val_steps, val_loss, label='Validation Loss', color='red', linewidth=2)\n",
    "    \n",
    "    plt.title('Training Stability: Loss vs. Steps')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq Transformer Translation Examples:\n",
      "\n",
      "Example 1:\n",
      "  EN: The implications are significant for the agro-food sector overall, which has been struck squarely by this crisis.\n",
      "  FR (true):  Les conséquences sont importantes pour l'ensemble de ce secteur agro-alimentaire qui a été touché de plein fouet par cette crise.\n",
      "  FR (pred):  Les implications sont importantes pour l'ensemble du secteur agro-alimentaire, qui a été frappé de manière directe par cette crise.\n",
      "\n",
      "Example 2:\n",
      "  EN: The EUR 614 million adopted by the Council will be enough to finance all the foreseeable needs.\n",
      "  FR (true):  Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "  FR (pred):  Les 614 millions d'euros adoptés par le Conseil suffiront à financer l'ensemble des besoins prévisibles.\n",
      "\n",
      "Example 3:\n",
      "  EN: Joint motion for a resolution (B5-0181/2000) on the shipwreck of the .\n",
      "  FR (true):  Proposition de résolution commune (B5-0181/2000) sur le naufrage de l'Erika.\n",
      "  FR (pred):  Proposition de résolution commune (B5-0181/2000) sur le naufrage du navire.\n",
      "\n",
      "Example 4:\n",
      "  EN: Following the debates with the Council last night I was wondering whether this report is actually close enough to the original proposal for first reading procedure to be possible.\n",
      "  FR (true):  À la suite aux débats du Conseil hier soir, je me demandais si ce rapport était en fait assez proche de la proposition originale pour permettre la procédure de première lecture.\n",
      "  FR (pred):  À la suite des débats qui ont eu lieu hier soir avec le Conseil, je me demandais si ce rapport était suffisamment proche de la proposition initiale de procédure de première lecture pour être possible.\n",
      "\n",
      "Example 5:\n",
      "  EN: Mr Prodi has spoken in favour of preserving the Community model, which met with great approval throughout this Chamber, and I would like, in my turn, to welcome this speech which will doubtless continue to be viewed as a major one.\n",
      "  FR (true):  M. Prodi, président de la Commission, a fait un plaidoyer pour la préservation du modèle communautaire, qui a recueilli un très fort assentiment sur tous ces bancs et je veux, à mon tour, saluer ce discours qui restera, sans nul doute, un discours important.\n",
      "  FR (pred):  M. Prodi s'est prononcé en faveur de la préservation du modèle communautaire, qui a reçu une grande approbation dans tout cet hémicycle, et je voudrais à mon tour saluer cette intervention qui restera sans aucun doute importante.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the seq2seq model on a few examples\n",
    "print(\"Seq2Seq Transformer Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = seq2seq_model.translate(en, num_beams=4)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "We'll evaluate all three models using BLEU score and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translations(model, test_pairs, model_name=\"Model\", max_samples=None):\n",
    "    \"\"\"Evaluate translation model on test set.\"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    if max_samples:\n",
    "        test_subset = test_pairs[:max_samples]\n",
    "    else:\n",
    "        test_subset = test_pairs\n",
    "    \n",
    "    print(f\"Evaluating {model_name} on {len(test_subset):,} test examples...\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for i, (en, fr_true) in enumerate(test_subset):\n",
    "        try:\n",
    "            fr_pred = model.translate(en)\n",
    "            predictions.append(fr_pred)\n",
    "            references.append(fr_true)\n",
    "            \n",
    "            # Tokenize for BLEU\n",
    "            pred_tokens = word_tokenize(fr_pred.lower())\n",
    "            ref_tokens = word_tokenize(fr_true.lower())\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            # Exact match\n",
    "            if fr_pred.lower().strip() == fr_true.lower().strip():\n",
    "                exact_matches += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error on example {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i+1:,} examples...\")\n",
    "    \n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match_rate = exact_matches / len(test_subset) if test_subset else 0.0\n",
    "    \n",
    "    results = {\n",
    "        'avg_bleu': avg_bleu,\n",
    "        'exact_match_rate': exact_match_rate,\n",
    "        'num_evaluated': len(test_subset),\n",
    "        'predictions': predictions,\n",
    "        'references': references\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"  Exact Match Rate: {exact_match_rate:.4f} ({exact_matches}/{len(test_subset)})\")\n",
    "\n",
    "    safe_name = model_name.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    save_path = os.path.join(save_dir, f\"{safe_name}_results.json\")\n",
    "    \n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    with open(save_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluating Baseline (Word-for-Word) on 1,000 test examples...\n",
      "  Processed 1,000 examples...\n",
      "\n",
      "Baseline (Word-for-Word) Results:\n",
      "  Average BLEU Score: 0.0369\n",
      "  Exact Match Rate: 0.0000 (0/1000)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model\n",
    "# Using a subset for faster evaluation (adjust as needed)\n",
    "print(\"=\" * 60)\n",
    "baseline_results = evaluate_translations(\n",
    "    baseline_model, \n",
    "    test_pairs, \n",
    "    model_name=\"Baseline (Word-for-Word)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluating Advanced (Cross-Lingual Embeddings) on 1,000 test examples...\n",
      "  Processed 1,000 examples...\n",
      "\n",
      "Advanced (Cross-Lingual Embeddings) Results:\n",
      "  Average BLEU Score: 0.0387\n",
      "  Exact Match Rate: 0.0020 (2/1000)\n",
      "============================================================\n",
      "Evaluating Best Model (Seq2Seq Transformer) on 1,000 test examples...\n",
      "  Processed 1,000 examples...\n",
      "\n",
      "Best Model (Seq2Seq Transformer) Results:\n",
      "  Average BLEU Score: 0.3136\n",
      "  Exact Match Rate: 0.0220 (22/1000)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate embedding model\n",
    "print(\"=\" * 60)\n",
    "embedding_results = evaluate_translations(\n",
    "    embedding_model,\n",
    "    test_pairs,\n",
    "    model_name=\"Advanced (Cross-Lingual Embeddings)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")\n",
    "\n",
    "# Evaluate seq2seq model\n",
    "print(\"=\" * 60)\n",
    "seq2seq_results = evaluate_translations(\n",
    "    seq2seq_model,\n",
    "    test_pairs,\n",
    "    model_name=\"Best Model (Seq2Seq Transformer)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON - ALL THREE MODELS\n",
      "================================================================================\n",
      "Metric                         Baseline             Embeddings           Seq2Seq             \n",
      "--------------------------------------------------------------------------------\n",
      "Average BLEU Score             0.0369               0.0387               0.3136              \n",
      "Exact Match Rate               0.0000               0.0020               0.0220              \n",
      "Number Evaluated               1000                 1000                 1000                \n",
      "\n",
      "Improvements over Baseline:   \n",
      "  Embeddings: +0.0018 (+4.79%)\n",
      "  Seq2Seq:    +0.2766 (+748.79%)\n",
      "\n",
      "Seq2Seq vs Embeddings: +0.2749 (+710.00%)\n",
      "\n",
      "🏆 Best Model: Seq2Seq (BLEU: 0.3136)\n"
     ]
    }
   ],
   "source": [
    "# Compare all three models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON - ALL THREE MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<30} {'Baseline':<20} {'Embeddings':<20} {'Seq2Seq':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Average BLEU Score':<30} {baseline_results['avg_bleu']:<20.4f} {embedding_results['avg_bleu']:<20.4f} {seq2seq_results['avg_bleu']:<20.4f}\")\n",
    "print(f\"{'Exact Match Rate':<30} {baseline_results['exact_match_rate']:<20.4f} {embedding_results['exact_match_rate']:<20.4f} {seq2seq_results['exact_match_rate']:<20.4f}\")\n",
    "print(f\"{'Number Evaluated':<30} {baseline_results['num_evaluated']:<20} {embedding_results['num_evaluated']:<20} {seq2seq_results['num_evaluated']:<20}\")\n",
    "\n",
    "# Calculate improvements\n",
    "embedding_improvement = embedding_results['avg_bleu'] - baseline_results['avg_bleu']\n",
    "seq2seq_improvement = seq2seq_results['avg_bleu'] - baseline_results['avg_bleu']\n",
    "seq2seq_vs_embedding = seq2seq_results['avg_bleu'] - embedding_results['avg_bleu']\n",
    "\n",
    "print(f\"\\n{'Improvements over Baseline:':<30}\")\n",
    "print(f\"  Embeddings: {embedding_improvement:+.4f} ({embedding_improvement/baseline_results['avg_bleu']*100:+.2f}%)\")\n",
    "print(f\"  Seq2Seq:    {seq2seq_improvement:+.4f} ({seq2seq_improvement/baseline_results['avg_bleu']*100:+.2f}%)\")\n",
    "print(f\"\\nSeq2Seq vs Embeddings: {seq2seq_vs_embedding:+.4f} ({seq2seq_vs_embedding/embedding_results['avg_bleu']*100:+.2f}%)\")\n",
    "\n",
    "# Determine winner\n",
    "best_model = max([\n",
    "    ('Baseline', baseline_results['avg_bleu']),\n",
    "    ('Embeddings', embedding_results['avg_bleu']),\n",
    "    ('Seq2Seq', seq2seq_results['avg_bleu'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model[0]} (BLEU: {best_model[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def plot_performance_by_length(model, test_pairs, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Performs Error Analysis: Plots BLEU score vs. Sentence Length.\n",
    "    \"\"\"\n",
    "    print(\"Running Error Analysis (Performance by Length)...\")\n",
    "    \n",
    "    # 1. Generate Data\n",
    "    lengths = []\n",
    "    scores = []\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Use a subset for speed\n",
    "    subset = test_pairs[:max_samples]\n",
    "    \n",
    "    for en, fr_true in subset:\n",
    "        try:\n",
    "            # Translate\n",
    "            fr_pred = model.translate(en)\n",
    "            if isinstance(fr_pred, list): fr_pred = fr_pred[0] # Handle embedding model\n",
    "            \n",
    "            # Tokenize\n",
    "            ref_tokens = word_tokenize(fr_true.lower())\n",
    "            pred_tokens = word_tokenize(fr_pred.lower())\n",
    "            \n",
    "            # Calculate BLEU\n",
    "            score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "            \n",
    "            # Record Length (English word count) and Score\n",
    "            lengths.append(len(en.split()))\n",
    "            scores.append(score)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 2. Create DataFrame for Analysis\n",
    "    df = pd.DataFrame({'length': lengths, 'bleu': scores})\n",
    "    \n",
    "    # 3. Bin the lengths (e.g., 0-10 words, 10-20 words...)\n",
    "    bins = [0, 10, 20, 30, 40, 50, 100]\n",
    "    labels = ['0-10', '10-20', '20-30', '30-40', '40-50', '50+']\n",
    "    df['length_bin'] = pd.cut(df['length'], bins=bins, labels=labels)\n",
    "    \n",
    "    # 4. Calculate Average BLEU per bin\n",
    "    bin_scores = df.groupby('length_bin')['bleu'].mean()\n",
    "    \n",
    "    # 5. Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bin_scores.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    \n",
    "    plt.title('Error Analysis: Translation Quality by Sentence Length')\n",
    "    plt.xlabel('Input Sentence Length (Words)')\n",
    "    plt.ylabel('Average BLEU Score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(bin_scores):\n",
    "        if not np.isnan(v):\n",
    "            plt.text(i, v + 0.01, f\"{v:.2f}\", ha='center')\n",
    "            \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- If the bar drops as length increases, your model struggles with long-term memory (common).\")\n",
    "    print(\"- If the first bar (0-10) is low, your model struggles with basic greetings/titles.\")\n",
    "\n",
    "# Run the analysis on your Best Model (Seq2Seq)\n",
    "# (Ensure seq2seq_model is loaded)\n",
    "plot_performance_by_length(seq2seq_model, test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Examples and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side-by-Side Translation Comparison (All Three Models):\n",
      "\n",
      "================================================================================\n",
      "Example 1\n",
      "================================================================================\n",
      "EN:  The implications are significant for the agro-food sector overall, which has been struck squarely by this crisis.\n",
      "\n",
      "FR (True):      Les conséquences sont importantes pour l'ensemble de ce secteur agro-alimentaire qui a été touché de plein fouet par cette crise.\n",
      "FR (Baseline):  La implications nous un pour la l la secteur de qui a été de elle par ce crise\n",
      "FR (Embedding): Ce sont justement les bénéfices qui sont au départ des crises actuelles au niveau de la sécurité alimentaire.\n",
      "FR (Seq2Seq):   Les implications sont importantes pour l'ensemble du secteur agro-alimentaire, qui a été frappé de manière directe par cette crise.\n",
      "\n",
      "================================================================================\n",
      "Example 2\n",
      "================================================================================\n",
      "EN:  The EUR 614 million adopted by the Council will be enough to finance all the foreseeable needs.\n",
      "\n",
      "FR (True):      Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "FR (Baseline):  La De 1999 millions la par la Conseil de être pas de des tous la à de\n",
      "FR (Embedding): Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "FR (Seq2Seq):   Les 614 millions d'euros adoptés par le Conseil suffiront à financer l'ensemble des besoins prévisibles.\n",
      "\n",
      "================================================================================\n",
      "Example 3\n",
      "================================================================================\n",
      "EN:  Joint motion for a resolution (B5-0181/2000) on the shipwreck of the .\n",
      "\n",
      "FR (True):      Proposition de résolution commune (B5-0181/2000) sur le naufrage de l'Erika.\n",
      "FR (Baseline):  De de pour un résolution B5 0181 2000 sur la naufrage de la\n",
      "FR (Embedding): Résolution commune sur le naufrage de l'Erika (RC B5-0181/2000)\n",
      "FR (Seq2Seq):   Proposition de résolution commune (B5-0181/2000) sur le naufrage du navire.\n",
      "\n",
      "================================================================================\n",
      "Example 4\n",
      "================================================================================\n",
      "EN:  Following the debates with the Council last night I was wondering whether this report is actually close enough to the original proposal for first reading procedure to be possible.\n",
      "\n",
      "FR (True):      À la suite aux débats du Conseil hier soir, je me demandais si ce rapport était en fait assez proche de la proposition originale pour permettre la procédure de première lecture.\n",
      "FR (Baseline):  La la débats avec la Conseil de de Je a de si ce rapport est en de pas de la la proposition pour première lecture de de être possible\n",
      "FR (Embedding): Lorsque j'ai rédigé ce rapport, j'y ai initialement introduit quelques idées polémiques afin de vérifier si on avait pris la peine de le lire.\n",
      "FR (Seq2Seq):   À la suite des débats qui ont eu lieu hier soir avec le Conseil, je me demandais si ce rapport était suffisamment proche de la proposition initiale de procédure de première lecture pour être possible.\n",
      "\n",
      "================================================================================\n",
      "Example 5\n",
      "================================================================================\n",
      "EN:  Mr Prodi has spoken in favour of preserving the Community model, which met with great approval throughout this Chamber, and I would like, in my turn, to welcome this speech which will doubtless continue to be viewed as a major one.\n",
      "\n",
      "FR (True):      M. Prodi, président de la Commission, a fait un plaidoyer pour la préservation du modèle communautaire, qui a recueilli un très fort assentiment sur tous ces bancs et je veux, à mon tour, saluer ce discours qui restera, sans nul doute, un discours important.\n",
      "FR (Baseline):  Monsieur Prodi a parlé dans en de préservation la La modèle qui et avec de l de ce Cette et Je je voudrais dans mon à de je ce de qui de sans à de être être comme un de un\n",
      "FR (Embedding): Je conclurai au nom de mon groupe en disant que M. Prodi a exposé aujourd'hui ce en quoi nous croyons.\n",
      "FR (Seq2Seq):   M. Prodi s'est prononcé en faveur de la préservation du modèle communautaire, qui a reçu une grande approbation dans tout cet hémicycle, et je voudrais à mon tour saluer cette intervention qui restera sans aucun doute importante.\n",
      "\n",
      "================================================================================\n",
      "Example 6\n",
      "================================================================================\n",
      "EN:  My visit to Iraq made me even more intolerant of the fact that, in the name of the defence of democracy and the rights of the Iraqi people, oppressed by Saddam Hussein, we are killing a people and destroying a country.\n",
      "\n",
      "FR (True):      Le voyage que j'ai fait en Irak a rendu encore plus inacceptable à mes yeux le fait qu'au nom de la défense de la démocratie et des droits de la population irakienne, opprimée par Saddam Hussein, on tue un peuple et on détruise un pays.\n",
      "FR (Baseline):  Mon visite de Irak de moi même plus et de la que que dans la nom de la de de démocratie et la droits de la Irakien de souffrances par Saddam Saddam nous nous de un de et des un pays\n",
      "FR (Embedding): Comme les autres orateurs, le sort de la population irakienne me préoccupe.\n",
      "FR (Seq2Seq):   Ma visite en Irak m'a rendu encore plus intolérant devant le fait que, au nom de la défense de la démocratie et des droits du peuple irakien, opprimé par Saddam Hussein, nous tuons un peuple et détruisons un pays.\n",
      "\n",
      "================================================================================\n",
      "Example 7\n",
      "================================================================================\n",
      "EN:  Parliament and the Council voted in favour of a text for the Directive on the patenting of biotechnological discoveries that completely rules out cloning of human beings, because this type of technology offends common decency and is an affront to law and order.\n",
      "\n",
      "FR (True):      Dans le contexte de la directive relative au brevetage des découvertes biotechnologiques, le Parlement et le Conseil ont approuvé un texte qui prévoit l'exclusion complète du clonage d'êtres humains, attendu que cette technique va à l'encontre des bonnes murs et de l'ordre public.\n",
      "FR (Baseline):  Parlement et la Conseil voté dans en de un texte pour la Directive sur la la de inventions le que à règles de clonage de de êtres parce ce de de de est la cohérence et est un un de droit et pour\n",
      "FR (Embedding): Cette affaire nous montre combien il était important, lors des discussions sur la directive relative à la protection juridique des inventions biotechnologiques, que le Parlement ait clairement ancré dans cette directive que les techniques de clonage de l' être humain, les techniques d' intervention sur la ligne germinale et, surtout, sur le corps humain dans toutes les phases de son développement sont exclues de la brevetabilité.\n",
      "FR (Seq2Seq):   Le Parlement et le Conseil ont voté en faveur d'un texte de la directive sur le brevetage des découvertes biotechnologiques qui exclut totalement le clonage des êtres humains, parce que ce type de technologie offense la décence commune et constitue un affront à l'ordre public.\n",
      "\n",
      "================================================================================\n",
      "Example 8\n",
      "================================================================================\n",
      "EN:  Let me repeat to the honourable Member that I have available a table which, in effect, shows that what I have said includes the disparities.\n",
      "\n",
      "FR (True):      Je répète à l'honorable parlementaire que je tiens à disposition un tableau qui montre effectivement que ce que j'ai dit recouvre des disparités.\n",
      "FR (Baseline):  Je moi je de la l États que Je nous de un la qui dans effet montre que ce Je nous dit de la les\n",
      "FR (Embedding): Je suis favorable à cette table ronde.\n",
      "FR (Seq2Seq):   Permettez-moi de répéter à l'honorable parlementaire que j'ai un tableau qui montre effectivement que ce que j'ai dit inclut les différences.\n",
      "\n",
      "================================================================================\n",
      "Example 9\n",
      "================================================================================\n",
      "EN:  I do not congratulate the rapporteur, Mrs Lalumiere;\n",
      "\n",
      "FR (True):      Pour ma part, je ne félicite pas le rapporteur Mme Lalumière.\n",
      "FR (Baseline):  Je ne pas féliciter la rapporteur Mme Lalumiere\n",
      "FR (Embedding): Comme cela n' a pas été le cas, je ne peux dès lors pas appuyer, en toute conscience, la position du rapporteur.\n",
      "FR (Seq2Seq):   Je ne félicite pas le rapporteur, Mme Lalumiere ;\n",
      "\n",
      "================================================================================\n",
      "Example 10\n",
      "================================================================================\n",
      "EN:  But whether or not that will be possible will only become apparent in the autumn of this year once we have the up-to-date progress reports on preparations for accession for each individual candidate country.\n",
      "\n",
      "FR (True):      Mais on ne saura si c'est possible que lorsqu'on disposera, à l'automne de cette année, du rapport actualisé sur les progrès des préparatifs en vue de l'adhésion pour chacun des pays candidats.\n",
      "FR (Baseline):  Mais si ou pas que de être possible de ne de que dans la automne de ce année une nous nous la de de à progrès rapports sur la pour adhésion pour chaque de pays pays\n",
      "FR (Embedding): Enfin, étant donné que je manque de chiffres concrets concernant la situation dans les pays candidats à l'adhésion, je voudrais vous demander s'il est possible d'effectuer des études sur le statu quo dans ces pays candidats et ce, le plus rapidement possible, afin de pouvoir déterminer si des améliorations interviennent.\n",
      "FR (Seq2Seq):   Mais la question de savoir si cela sera possible ne sera évidente qu'à l'automne de cette année, lorsque nous disposerons des rapports d'étape à jour sur la préparation de l'adhésion de chaque pays candidat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show side-by-side comparisons for all three models\n",
    "print(\"Side-by-Side Translation Comparison (All Three Models):\\n\")\n",
    "num_examples = 10\n",
    "for i in range(min(num_examples, len(test_pairs))):\n",
    "    en, fr_true = test_pairs[i]\n",
    "    fr_baseline = baseline_model.translate_preserve_case(en)\n",
    "    fr_embedding = embedding_model.translate(en)\n",
    "    fr_seq2seq = seq2seq_model.translate(en, num_beams=4)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EN:  {en}\")\n",
    "    print(f\"\\nFR (True):      {fr_true}\")\n",
    "    print(f\"FR (Baseline):  {fr_baseline}\")\n",
    "    print(f\"FR (Embedding): {fr_embedding}\")\n",
    "    print(f\"FR (Seq2Seq):   {fr_seq2seq}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_attention_map(model_wrapper, text):\n",
    "    \"\"\"\n",
    "    Generates an attention heatmap for the Cross-Attention layer of the decoder.\n",
    "    This shows which English input words the model focuses on when generating each French word.\n",
    "    \"\"\"\n",
    "    model = model_wrapper.model\n",
    "    tokenizer = model_wrapper.tokenizer\n",
    "    device = model_wrapper.device\n",
    "    \n",
    "    # 1. Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 2. Generate output with attentions\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_length=50\n",
    "        )\n",
    "    \n",
    "    # 3. Process Tokens\n",
    "    # Get input tokens (English)\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "    \n",
    "    # Get output tokens (French) - skip the first token (usually start-of-sequence)\n",
    "    output_ids = outputs.sequences[0]\n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(output_ids)\n",
    "    \n",
    "    # 4. Extract Attention\n",
    "    # outputs.cross_attentions is a tuple of tuples (one per generation step)\n",
    "    # We want the last layer's attention for each step\n",
    "    # Shape: (num_generated_tokens, num_heads, input_length)\n",
    "    \n",
    "    attention_matrix = []\n",
    "    \n",
    "    for step_attentions in outputs.cross_attentions:\n",
    "        # step_attentions contains attentions for all layers at this step\n",
    "        # Get last layer [-1], squeeze batch dim [0]\n",
    "        # Shape: (num_heads, 1, input_seq_len) -> Average over heads\n",
    "        last_layer_attn = step_attentions[-1][0] \n",
    "        avg_attn = last_layer_attn.mean(dim=0).cpu().numpy() # Average over heads\n",
    "        attention_matrix.append(avg_attn)\n",
    "    \n",
    "    # Convert to numpy array (Output Length x Input Length)\n",
    "    attention_matrix = np.array(attention_matrix)\n",
    "    \n",
    "    # Remove padding tokens from visualization if desired\n",
    "    # (Simple slicing based on token lists)\n",
    "    \n",
    "    # 5. Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        attention_matrix,\n",
    "        xticklabels=input_tokens,\n",
    "        yticklabels=output_tokens[1:], # Offset by 1 because cross_attentions starts at 1st generated token\n",
    "        cmap=\"Viridis\",\n",
    "        cbar=True\n",
    "    )\n",
    "    plt.title(f\"Attention Map: '{text}'\")\n",
    "    plt.xlabel(\"Source (English)\")\n",
    "    plt.ylabel(\"Generated Target (French)\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "# Test on a specific example\n",
    "test_sentence = \"The European Parliament must vote on this law.\"\n",
    "print(f\"Visualizing attention for: {test_sentence}\")\n",
    "plot_attention_map(seq2seq_model, test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented and compared two translation approaches:\n",
    "\n",
    "1. **Baseline (Word-for-Word)**: Simple dictionary-based translation that maps each English word to its most common French translation from the training data.\n",
    "\n",
    "2. **Advanced (Cross-Lingual Embeddings)**: Uses multilingual sentence embeddings to find the most semantically similar French sentence from the training corpus.\n",
    "\n",
    "The cross-lingual embedding approach should generally perform better as it considers semantic meaning rather than just word-level mappings. However, it requires more computational resources and may be slower for large candidate pools.\n",
    "\n",
    "### Future Improvements:\n",
    "- Use more sophisticated word alignment algorithms (e.g., IBM models)\n",
    "- Implement phrase-based translation\n",
    "- Use neural sequence-to-sequence models\n",
    "- Fine-tune the embedding model on the specific domain\n",
    "- Add more evaluation metrics (METEOR, ROUGE, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
