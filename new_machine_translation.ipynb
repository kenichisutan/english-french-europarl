{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation: English-French Europarl\n",
    "\n",
    "This notebook implements three translation approaches:\n",
    "1. **Baseline**: Word-for-word translation using a bilingual dictionary\n",
    "2. **Advanced**: Cross-lingual embeddings for semantic translation\n",
    "3. **Best Model**: Fine-tuned Seq2Seq Transformer (iterative improvement)\n",
    "\n",
    "We'll evaluate all models on a train/test split of the Europarl corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_libraries = [\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",         \n",
    "    \"nltk\",\n",
    "    \"sentence-transformers\",\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"accelerate>=0.26.0\",   \n",
    "    \"sentencepiece\",\n",
    "    \"packaging\",\n",
    "    \"torch\"\n",
    "]\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + required_libraries)\n",
    "    print(\"All libraries installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Installation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Setup Paths\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "DATA_DIR = NOTEBOOK_DIR / \"data\"\n",
    "\n",
    "# Define the specific files you have\n",
    "EN_PATH = DATA_DIR / \"Europarl.en-fr.en\"\n",
    "FR_PATH = DATA_DIR / \"Europarl.en-fr.fr\"\n",
    "\n",
    "# Verify they exist\n",
    "if EN_PATH.exists() and FR_PATH.exists():\n",
    "    print(f\"Found English file: {EN_PATH.name}\")\n",
    "    print(f\"Found French file:  {FR_PATH.name}\")\n",
    "else:\n",
    "    print(\"Error: Files not found. Check your folder structure.\")\n",
    "\n",
    "def load_parallel_subset(en_path, fr_path, max_lines):\n",
    "    \"\"\"\n",
    "    Reads two aligned text files and returns a Pandas DataFrame with columns ['en', 'fr'].\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    try:\n",
    "        # Open both files at the same time\n",
    "        with open(en_path, \"r\", encoding=\"utf-8\") as f_en, \\\n",
    "             open(fr_path, \"r\", encoding=\"utf-8\") as f_fr:\n",
    "            \n",
    "            # zip() pairs lines together safely\n",
    "            for i, (line_en, line_fr) in enumerate(zip(f_en, f_fr)):\n",
    "                if i >= max_lines:\n",
    "                    break\n",
    "                \n",
    "                text_en = line_en.strip()\n",
    "                text_fr = line_fr.strip()\n",
    "                \n",
    "                # Only keep if both sides have content\n",
    "                if text_en and text_fr:\n",
    "                    pairs.append((text_en, text_fr))\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DF on error\n",
    "\n",
    "    # Convert list of tuples to DataFrame\n",
    "    df = pd.DataFrame(pairs, columns=[\"en\", \"fr\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 100000\n",
    "\n",
    "# Run the loader\n",
    "df = load_parallel_subset(EN_PATH, FR_PATH, max_lines=subset_size)\n",
    "print(f\"Successfully loaded {len(df)} pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the new loader\n",
    "df = load_parallel_subset(EN_PATH, FR_PATH, max_lines=subset_size)\n",
    "\n",
    "# --- THE FIX ---\n",
    "# Convert back to list of tuples for your old analysis functions\n",
    "pairs = df.to_records(index=False).tolist()\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Pairs list length: {len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets\n",
    "train_pairs, test_pairs = train_test_split(\n",
    "    pairs, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True \n",
    ")\n",
    "\n",
    "train_pairs[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: Word-for-Word Translation\n",
    "\n",
    "This baseline model builds a bilingual dictionary from the training data and translates each word independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordForWordTranslator:\n",
    "    \"\"\"Baseline word-for-word translation using bilingual dictionary.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_dict = defaultdict(lambda: defaultdict(int))\n",
    "        self.most_common_translations = {}\n",
    "        \n",
    "    def train(self, train_pairs):\n",
    "        \"\"\"Build bilingual dictionary from training pairs.\"\"\"\n",
    "        print(\"Building bilingual dictionary...\")\n",
    "        \n",
    "        for en_text, fr_text in train_pairs:\n",
    "            # Simple tokenization (split on whitespace and punctuation)\n",
    "            en_words = re.findall(r'\\b\\w+\\b', en_text.lower())\n",
    "            fr_words = re.findall(r'\\b\\w+\\b', fr_text.lower())\n",
    "            \n",
    "            if not en_words or not fr_words:\n",
    "                continue\n",
    "            \n",
    "            # Use positional alignment based on relative position\n",
    "            # This prevents common words like \"de\" from dominating\n",
    "            en_len = len(en_words)\n",
    "            fr_len = len(fr_words)\n",
    "            \n",
    "            # Align words based on their relative positions\n",
    "            for i, en_word in enumerate(en_words):\n",
    "                # Map English position to French position\n",
    "                fr_pos = int((i / en_len) * fr_len)\n",
    "                fr_pos = min(fr_pos, fr_len - 1)  # Ensure valid index\n",
    "                \n",
    "                # Align to the word at the corresponding position\n",
    "                fr_word = fr_words[fr_pos]\n",
    "                self.word_dict[en_word][fr_word] += 1\n",
    "                \n",
    "                # Also align to nearby words (within 1 position) for better coverage\n",
    "                if fr_pos > 0:\n",
    "                    self.word_dict[en_word][fr_words[fr_pos - 1]] += 0.5\n",
    "                if fr_pos < fr_len - 1:\n",
    "                    self.word_dict[en_word][fr_words[fr_pos + 1]] += 0.5\n",
    "        \n",
    "        # Store most common translation for each English word\n",
    "        for en_word, fr_translations in self.word_dict.items():\n",
    "            if fr_translations:\n",
    "                self.most_common_translations[en_word] = max(\n",
    "                    fr_translations.items(), \n",
    "                    key=lambda x: x[1]\n",
    "                )[0]\n",
    "        \n",
    "        print(f\"Dictionary built with {len(self.most_common_translations):,} English words\")\n",
    "        \n",
    "    def translate(self, en_text):\n",
    "        \"\"\"Translate English text word-by-word.\"\"\"\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en_text)\n",
    "        fr_words = []\n",
    "        \n",
    "        for word in en_words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in self.most_common_translations:\n",
    "                fr_words.append(self.most_common_translations[word_lower])\n",
    "            else:\n",
    "                # Unknown word - keep original\n",
    "                fr_words.append(word)\n",
    "        \n",
    "        return ' '.join(fr_words)\n",
    "    \n",
    "    def translate_preserve_case(self, en_text):\n",
    "        \"\"\"Translate preserving original word casing.\"\"\"\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en_text)\n",
    "        fr_words = []\n",
    "        \n",
    "        for word in en_words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in self.most_common_translations:\n",
    "                translation = self.most_common_translations[word_lower]\n",
    "                # Preserve case\n",
    "                if word[0].isupper():\n",
    "                    translation = translation.capitalize()\n",
    "                fr_words.append(translation)\n",
    "            else:\n",
    "                fr_words.append(word)\n",
    "        \n",
    "        return ' '.join(fr_words)\n",
    "\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = WordForWordTranslator()\n",
    "baseline_model.train(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the baseline model on a few examples\n",
    "print(\"Baseline Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = baseline_model.translate_preserve_case(en)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Model: Cross-Lingual Embeddings\n",
    "\n",
    "This model uses pre-trained multilingual embeddings to find the best translation by comparing semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class CrossLingualEmbeddingTranslator:\n",
    "    \"\"\"Translation using cross-lingual embeddings and semantic similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with a multilingual sentence transformer.\n",
    "        Options:\n",
    "        - 'paraphrase-multilingual-MiniLM-L12-v2' (fast, good quality)\n",
    "        - 'paraphrase-multilingual-mpnet-base-v2' (slower, better quality)\n",
    "        - 'distiluse-base-multilingual-cased' (alternative)\n",
    "        \"\"\"\n",
    "        print(f\"Loading multilingual embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.french_candidates = []\n",
    "        self.french_embeddings = None\n",
    "        \n",
    "    def train(self, train_pairs):\n",
    "        \"\"\"Build a candidate pool of French translations from training data.\"\"\"\n",
    "        print(\"Building French candidate pool...\")\n",
    "        \n",
    "        # Collect unique French sentences (or a large sample)\n",
    "        french_sentences = set()\n",
    "        for _, fr_text in train_pairs:\n",
    "            if fr_text.strip():\n",
    "                french_sentences.add(fr_text.strip())\n",
    "        \n",
    "        # Limit to reasonable size for efficiency (can be adjusted)\n",
    "        max_candidates = 50000\n",
    "        if len(french_sentences) > max_candidates:\n",
    "            french_sentences = list(french_sentences)[:max_candidates]\n",
    "        else:\n",
    "            french_sentences = list(french_sentences)\n",
    "        \n",
    "        self.french_candidates = french_sentences\n",
    "        \n",
    "        # Pre-compute embeddings for all French candidates\n",
    "        print(f\"Computing embeddings for {len(self.french_candidates):,} French candidates...\")\n",
    "        self.french_embeddings = self.model.encode(\n",
    "            self.french_candidates,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "    def translate(self, en_text, top_k=1):\n",
    "        \"\"\"\n",
    "        Translate by finding the most semantically similar French sentence.\n",
    "        \n",
    "        Args:\n",
    "            en_text: English text to translate\n",
    "            top_k: Number of top candidates to return (default: 1, returns best match)\n",
    "        \"\"\"\n",
    "        if not en_text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Get embedding for English text\n",
    "        en_embedding = self.model.encode([en_text])\n",
    "        \n",
    "        # Compute cosine similarity with all French candidates\n",
    "        similarities = cosine_similarity(en_embedding, self.french_embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        if top_k == 1:\n",
    "            return self.french_candidates[top_indices[0]]\n",
    "        else:\n",
    "            return [self.french_candidates[idx] for idx in top_indices]\n",
    "\n",
    "\n",
    "# Train the cross-lingual embedding model\n",
    "print(\"Training cross-lingual embedding model...\")\n",
    "embedding_model = CrossLingualEmbeddingTranslator()\n",
    "embedding_model.train(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_semantics(model, words_en, words_fr):\n",
    "    \"\"\"\n",
    "    Visualizes English and French word pairs in 2D space using PCA.\n",
    "    \"\"\"\n",
    "    # 1. Compute embeddings\n",
    "    embeddings_en = model.model.encode(words_en)\n",
    "    embeddings_fr = model.model.encode(words_fr)\n",
    "    \n",
    "    # 2. Combine for PCA\n",
    "    all_embeddings = np.vstack([embeddings_en, embeddings_fr])\n",
    "    \n",
    "    # 3. Reduce to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    results = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot English words (Blue)\n",
    "    plt.scatter(results[:len(words_en), 0], results[:len(words_en), 1], c='blue', label='English')\n",
    "    for i, word in enumerate(words_en):\n",
    "        plt.annotate(word, xy=(results[i, 0], results[i, 1]), xytext=(5, 2), \n",
    "                     textcoords='offset points', color='blue')\n",
    "                     \n",
    "    # Plot French words (Red)\n",
    "    plt.scatter(results[len(words_en):, 0], results[len(words_en):, 1], c='red', label='French')\n",
    "    for i, word in enumerate(words_fr):\n",
    "        plt.annotate(word, xy=(results[len(words_en)+i, 0], results[len(words_en)+i, 1]), \n",
    "                     xytext=(5, 2), textcoords='offset points', color='red')\n",
    "    \n",
    "    # Draw lines between pairs to show alignment\n",
    "    for i in range(len(words_en)):\n",
    "        plt.plot([results[i, 0], results[len(words_en)+i, 0]],\n",
    "                 [results[i, 1], results[len(words_en)+i, 1]], \n",
    "                 'k--', alpha=0.2)\n",
    "\n",
    "    plt.title('Cross-Lingual Semantic Space (PCA)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Define pairs to visualize (conceptually similar words should be close)\n",
    "concepts = [\n",
    "    (\"democracy\", \"d√©mocratie\"),\n",
    "    (\"parliament\", \"parlement\"),\n",
    "    (\"law\", \"loi\"),\n",
    "    (\"commission\", \"commission\"),\n",
    "    (\"vote\", \"vote\"),\n",
    "    (\"president\", \"pr√©sident\"),\n",
    "    (\"money\", \"argent\"),\n",
    "    (\"crisis\", \"crise\")\n",
    "]\n",
    "\n",
    "words_en = [c[0] for c in concepts]\n",
    "words_fr = [c[1] for c in concepts]\n",
    "\n",
    "visualize_semantics(embedding_model, words_en, words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the embedding model on a few examples\n",
    "print(\"Cross-Lingual Embedding Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = embedding_model.translate(en)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Model: Fine-tuned Seq2Seq Transformer\n",
    "\n",
    "This is our iterative improvement model. We'll fine-tune a pretrained transformer model on our Europarl data to achieve the best translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set device (CUDA, Apple Silicon MPS, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device} (CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    " )\n",
    "from datasets import Dataset\n",
    "\n",
    "class Seq2SeqTranslator:\n",
    "    \"\"\"Fine-tuned Seq2Seq Transformer for English-French translation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='Helsinki-NLP/opus-mt-en-fr', max_length=128):\n",
    "        \"\"\"\n",
    "        Initialize with a pretrained translation model.\n",
    "        \n",
    "        Options:\n",
    "        - 'Helsinki-NLP/opus-mt-en-fr' (fast, good baseline, ~300MB)\n",
    "        - 'facebook/mbart-large-50' (multilingual, better quality, ~2.5GB, needs more GPU)\n",
    "        - 'google/mt5-base' (multilingual T5, good quality, ~850MB)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure sentencepiece is installed (required for opus-mt and some other models)\n",
    "        try:\n",
    "            import sentencepiece\n",
    "        except ImportError:\n",
    "            print(\"Installing sentencepiece (required for this tokenizer)...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sentencepiece'])\n",
    "            import sentencepiece\n",
    "            print(\"sentencepiece installed successfully!\")\n",
    "        \n",
    "        print(f\"Initializing model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "        \n",
    "    def train(self, train_pairs, val_pairs=None, \n",
    "              num_epochs=3, batch_size=8, learning_rate=5e-5,\n",
    "              save_steps=1000, eval_steps=500, warmup_steps=500):\n",
    "        \"\"\"\n",
    "        Fine-tune the model on training data.\n",
    "        \n",
    "        Args:\n",
    "            train_pairs: List of (en, fr) tuples for training\n",
    "            val_pairs: Optional validation set (if None, uses 10% of train)\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Training batch size (adjust based on GPU memory)\n",
    "            learning_rate: Learning rate for fine-tuning\n",
    "            save_steps: Save checkpoint every N steps\n",
    "            eval_steps: Evaluate every N steps\n",
    "            warmup_steps: Warmup steps for learning rate scheduler\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Training Seq2Seq Transformer Model\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Training examples: {len(train_pairs):,}\")\n",
    "\n",
    "        import json \n",
    "        import os  \n",
    "\n",
    "        # Prepare validation set\n",
    "        if val_pairs is None:\n",
    "            # Use 10% of training data for validation\n",
    "            val_size = max(1000, len(train_pairs) // 10)\n",
    "            val_pairs = train_pairs[:val_size]\n",
    "            train_pairs = train_pairs[val_size:]\n",
    "            print(f\"Split: {len(train_pairs):,} train, {len(val_pairs):,} validation\")\n",
    "        \n",
    "        # Convert to HuggingFace Dataset format\n",
    "        def prepare_dataset(pairs):\n",
    "            return Dataset.from_dict({\n",
    "                'en': [pair[0] for pair in pairs],\n",
    "                'fr': [pair[1] for pair in pairs]\n",
    "            })\n",
    "        \n",
    "        train_dataset = prepare_dataset(train_pairs)\n",
    "        val_dataset = prepare_dataset(val_pairs)\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        def tokenize_function(examples):\n",
    "            # Tokenize English (source) and French (target)\n",
    "            model_inputs = self.tokenizer(\n",
    "                examples['en'],\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            \n",
    "            # Tokenize French targets\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                labels = self.tokenizer(\n",
    "                    examples['fr'],\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding='max_length'\n",
    "                )\n",
    "            \n",
    "            # Replace padding token id's of the labels with -100 (ignored by loss)\n",
    "            labels['input_ids'] = [\n",
    "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels['input_ids']\n",
    "            ]\n",
    "            \n",
    "            model_inputs['labels'] = labels['input_ids']\n",
    "            return model_inputs\n",
    "        \n",
    "        print(\"Tokenizing datasets...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        val_dataset = val_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        output_dir = f\"./seq2seq_model_{self.model_name.split('/')[-1]}\"\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f'{output_dir}/logs',\n",
    "            logging_steps=100,\n",
    "            eval_steps=eval_steps,\n",
    "            save_steps=save_steps,\n",
    "            eval_strategy=\"steps\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            fp16=torch.cuda.is_available(),  # Use mixed precision if CUDA GPU available (MPS doesn't support fp16)\n",
    "            report_to=\"none\",  # Disable wandb/tensorboard\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Train!\n",
    "        print(\"\\nStarting training...\")\n",
    "        print(f\"Epochs: {num_epochs}, Batch size: {batch_size}, Learning rate: {learning_rate}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: CUDA available\")\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            print(f\"GPU: Apple Silicon (MPS) available\")\n",
    "        else:\n",
    "            print(f\"Using: CPU\")\n",
    "        \n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "        log_history_path = os.path.join(output_dir, \"training_logs.json\")\n",
    "        with open(log_history_path, \"w\") as f:\n",
    "            json.dump(trainer.state.log_history, f)\n",
    "        print(f\"Training logs saved to {log_history_path}\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def translate(self, en_text, max_length=None, num_beams=4, do_sample=False):\n",
    "        \"\"\"\n",
    "        Translate English text to French.\n",
    "        \n",
    "        Args:\n",
    "            en_text: English text to translate\n",
    "            max_length: Maximum output length (default: self.max_length)\n",
    "            num_beams: Number of beams for beam search (higher = better quality, slower)\n",
    "            do_sample: Whether to use sampling (False = deterministic)\n",
    "        \"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            en_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=do_sample,\n",
    "                early_stopping=True,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation\n",
    "    \n",
    "    def translate_batch(self, en_texts, batch_size=8, **kwargs):\n",
    "        \"\"\"Translate a batch of English texts.\"\"\"\n",
    "        translations = []\n",
    "        for i in range(0, len(en_texts), batch_size):\n",
    "            batch = en_texts[i:i+batch_size]\n",
    "            batch_translations = [self.translate(text, **kwargs) for text in batch]\n",
    "            translations.extend(batch_translations)\n",
    "        return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the seq2seq model\n",
    "# You can adjust the model_name to try different pretrained models\n",
    "# For faster training/testing, start with opus-mt-en-fr\n",
    "# For better quality (if you have GPU memory), try mBART or mT5\n",
    "\n",
    "seq2seq_model = Seq2SeqTranslator(\n",
    "    model_name='Helsinki-NLP/opus-mt-en-fr',  # Good baseline, fast\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(\"\\nModel initialized and ready for training!\")\n",
    "print(\"Note: Training will take time. Adjust batch_size and num_epochs based on your GPU memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "**Adjust these parameters based on your resources:**\n",
    "\n",
    "- **CUDA GPU (NVIDIA)**: Use larger batch_size (16-32), more epochs (3-5), fp16 enabled\n",
    "- **Apple Silicon GPU (M1/M2/M3)**: Use moderate batch_size (8-16), more epochs (3-5), no fp16\n",
    "- **CPU only**: Use smaller batch_size (2-4), fewer epochs (1-2), expect slower training\n",
    "- **Limited GPU memory**: Reduce batch_size, use gradient accumulation\n",
    "\n",
    "**Model options:**\n",
    "- `Helsinki-NLP/opus-mt-en-fr`: Fast, ~300MB, good for quick iteration\n",
    "- `facebook/mbart-large-50`: Better quality, ~2.5GB, needs more GPU memory\n",
    "- `google/mt5-base`: Good balance, ~850MB\n",
    "\n",
    "**Note for Apple Silicon (MacBook):**\n",
    "- The code automatically detects and uses MPS (Metal Performance Shaders)\n",
    "- Training will be faster than CPU but may be slower than high-end NVIDIA GPUs\n",
    "- Some operations may fall back to CPU if not supported by MPS\n",
    "- Mixed precision (fp16) is not supported on MPS, so training uses full precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the seq2seq model\n",
    "# Adjust parameters based on your GPU/CPU and time constraints\n",
    "\n",
    "# For quick testing (CPU or limited time):\n",
    "# seq2seq_model.train(\n",
    "#     train_pairs,\n",
    "#     num_epochs=1,\n",
    "#     batch_size=4 if not torch.cuda.is_available() else 8,\n",
    "#     learning_rate=5e-5,\n",
    "#     save_steps=500,\n",
    "#     eval_steps=250\n",
    "# )\n",
    "\n",
    "# For better results (with GPU):\n",
    "# Determine batch size based on available device\n",
    "if torch.cuda.is_available():\n",
    "    batch_size = 16  # CUDA GPU - can use larger batches\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    batch_size = 8   # Apple Silicon GPU - moderate batch size\n",
    "else:\n",
    "    batch_size = 4   # CPU - smaller batches\n",
    "\n",
    "trainer = seq2seq_model.train(\n",
    "    train_pairs,\n",
    "    num_epochs=3,  # Increase to 5-10 for better results\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=5e-5,  # Try 3e-5 or 1e-4 for experimentation\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    warmup_steps=500\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Model is ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss from the HuggingFace Trainer history.\n",
    "    \"\"\"\n",
    "    # Retrieve logs\n",
    "    history = trainer.state.log_history\n",
    "    \n",
    "    # Extract loss values\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_steps = []\n",
    "    val_steps = []\n",
    "    \n",
    "    for entry in history:\n",
    "        if 'loss' in entry:\n",
    "            train_loss.append(entry['loss'])\n",
    "            train_steps.append(entry['step'])\n",
    "        if 'eval_loss' in entry:\n",
    "            val_loss.append(entry['eval_loss'])\n",
    "            val_steps.append(entry['step'])\n",
    "            \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_steps, train_loss, label='Training Loss', color='blue', alpha=0.6)\n",
    "    plt.plot(val_steps, val_loss, label='Validation Loss', color='red', linewidth=2)\n",
    "    \n",
    "    plt.title('Training Stability: Loss vs. Steps')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the seq2seq model on a few examples\n",
    "print(\"Seq2Seq Transformer Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = seq2seq_model.translate(en, num_beams=4)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "We'll evaluate all three models using BLEU score and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translations(model, test_pairs, model_name=\"Model\", max_samples=None):\n",
    "    \"\"\"Evaluate translation model on test set.\"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    if max_samples:\n",
    "        test_subset = test_pairs[:max_samples]\n",
    "    else:\n",
    "        test_subset = test_pairs\n",
    "    \n",
    "    print(f\"Evaluating {model_name} on {len(test_subset):,} test examples...\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for i, (en, fr_true) in enumerate(test_subset):\n",
    "        try:\n",
    "            fr_pred = model.translate(en)\n",
    "            predictions.append(fr_pred)\n",
    "            references.append(fr_true)\n",
    "            \n",
    "            # Tokenize for BLEU\n",
    "            pred_tokens = word_tokenize(fr_pred.lower())\n",
    "            ref_tokens = word_tokenize(fr_true.lower())\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            # Exact match\n",
    "            if fr_pred.lower().strip() == fr_true.lower().strip():\n",
    "                exact_matches += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error on example {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i+1:,} examples...\")\n",
    "    \n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match_rate = exact_matches / len(test_subset) if test_subset else 0.0\n",
    "    \n",
    "    results = {\n",
    "        'avg_bleu': avg_bleu,\n",
    "        'exact_match_rate': exact_match_rate,\n",
    "        'num_evaluated': len(test_subset),\n",
    "        'predictions': predictions,\n",
    "        'references': references\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"  Exact Match Rate: {exact_match_rate:.4f} ({exact_matches}/{len(test_subset)})\")\n",
    "\n",
    "    safe_name = model_name.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    save_path = os.path.join(save_dir, f\"{safe_name}_results.json\")\n",
    "    \n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "\n",
    "    with open(save_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "# Using a subset for faster evaluation (adjust as needed)\n",
    "print(\"=\" * 60)\n",
    "baseline_results = evaluate_translations(\n",
    "    baseline_model, \n",
    "    test_pairs, \n",
    "    model_name=\"Baseline (Word-for-Word)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate embedding model\n",
    "print(\"=\" * 60)\n",
    "embedding_results = evaluate_translations(\n",
    "    embedding_model,\n",
    "    test_pairs,\n",
    "    model_name=\"Advanced (Cross-Lingual Embeddings)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")\n",
    "\n",
    "# Evaluate seq2seq model\n",
    "print(\"=\" * 60)\n",
    "seq2seq_results = evaluate_translations(\n",
    "    seq2seq_model,\n",
    "    test_pairs,\n",
    "    model_name=\"Best Model (Seq2Seq Transformer)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON - ALL THREE MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<30} {'Baseline':<20} {'Embeddings':<20} {'Seq2Seq':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Average BLEU Score':<30} {baseline_results['avg_bleu']:<20.4f} {embedding_results['avg_bleu']:<20.4f} {seq2seq_results['avg_bleu']:<20.4f}\")\n",
    "print(f\"{'Exact Match Rate':<30} {baseline_results['exact_match_rate']:<20.4f} {embedding_results['exact_match_rate']:<20.4f} {seq2seq_results['exact_match_rate']:<20.4f}\")\n",
    "print(f\"{'Number Evaluated':<30} {baseline_results['num_evaluated']:<20} {embedding_results['num_evaluated']:<20} {seq2seq_results['num_evaluated']:<20}\")\n",
    "\n",
    "# Calculate improvements\n",
    "embedding_improvement = embedding_results['avg_bleu'] - baseline_results['avg_bleu']\n",
    "seq2seq_improvement = seq2seq_results['avg_bleu'] - baseline_results['avg_bleu']\n",
    "seq2seq_vs_embedding = seq2seq_results['avg_bleu'] - embedding_results['avg_bleu']\n",
    "\n",
    "print(f\"\\n{'Improvements over Baseline:':<30}\")\n",
    "print(f\"  Embeddings: {embedding_improvement:+.4f} ({embedding_improvement/baseline_results['avg_bleu']*100:+.2f}%)\")\n",
    "print(f\"  Seq2Seq:    {seq2seq_improvement:+.4f} ({seq2seq_improvement/baseline_results['avg_bleu']*100:+.2f}%)\")\n",
    "print(f\"\\nSeq2Seq vs Embeddings: {seq2seq_vs_embedding:+.4f} ({seq2seq_vs_embedding/embedding_results['avg_bleu']*100:+.2f}%)\")\n",
    "\n",
    "# Determine winner\n",
    "best_model = max([\n",
    "    ('Baseline', baseline_results['avg_bleu']),\n",
    "    ('Embeddings', embedding_results['avg_bleu']),\n",
    "    ('Seq2Seq', seq2seq_results['avg_bleu'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model[0]} (BLEU: {best_model[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def plot_performance_by_length(model, test_pairs, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Performs Error Analysis: Plots BLEU score vs. Sentence Length.\n",
    "    \"\"\"\n",
    "    print(\"Running Error Analysis (Performance by Length)...\")\n",
    "    \n",
    "    # 1. Generate Data\n",
    "    lengths = []\n",
    "    scores = []\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Use a subset for speed\n",
    "    subset = test_pairs[:max_samples]\n",
    "    \n",
    "    for en, fr_true in subset:\n",
    "        try:\n",
    "            # Translate\n",
    "            fr_pred = model.translate(en)\n",
    "            if isinstance(fr_pred, list): fr_pred = fr_pred[0] # Handle embedding model\n",
    "            \n",
    "            # Tokenize\n",
    "            ref_tokens = word_tokenize(fr_true.lower())\n",
    "            pred_tokens = word_tokenize(fr_pred.lower())\n",
    "            \n",
    "            # Calculate BLEU\n",
    "            score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "            \n",
    "            # Record Length (English word count) and Score\n",
    "            lengths.append(len(en.split()))\n",
    "            scores.append(score)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 2. Create DataFrame for Analysis\n",
    "    df = pd.DataFrame({'length': lengths, 'bleu': scores})\n",
    "    \n",
    "    # 3. Bin the lengths (e.g., 0-10 words, 10-20 words...)\n",
    "    bins = [0, 10, 20, 30, 40, 50, 100]\n",
    "    labels = ['0-10', '10-20', '20-30', '30-40', '40-50', '50+']\n",
    "    df['length_bin'] = pd.cut(df['length'], bins=bins, labels=labels)\n",
    "    \n",
    "    # 4. Calculate Average BLEU per bin\n",
    "    bin_scores = df.groupby('length_bin')['bleu'].mean()\n",
    "    \n",
    "    # 5. Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bin_scores.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    \n",
    "    plt.title('Error Analysis: Translation Quality by Sentence Length')\n",
    "    plt.xlabel('Input Sentence Length (Words)')\n",
    "    plt.ylabel('Average BLEU Score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(bin_scores):\n",
    "        if not np.isnan(v):\n",
    "            plt.text(i, v + 0.01, f\"{v:.2f}\", ha='center')\n",
    "            \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- If the bar drops as length increases, your model struggles with long-term memory (common).\")\n",
    "    print(\"- If the first bar (0-10) is low, your model struggles with basic greetings/titles.\")\n",
    "\n",
    "# Run the analysis on your Best Model (Seq2Seq)\n",
    "# (Ensure seq2seq_model is loaded)\n",
    "plot_performance_by_length(seq2seq_model, test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Examples and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show side-by-side comparisons for all three models\n",
    "print(\"Side-by-Side Translation Comparison (All Three Models):\\n\")\n",
    "num_examples = 10\n",
    "for i in range(min(num_examples, len(test_pairs))):\n",
    "    en, fr_true = test_pairs[i]\n",
    "    fr_baseline = baseline_model.translate_preserve_case(en)\n",
    "    fr_embedding = embedding_model.translate(en)\n",
    "    fr_seq2seq = seq2seq_model.translate(en, num_beams=4)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EN:  {en}\")\n",
    "    print(f\"\\nFR (True):      {fr_true}\")\n",
    "    print(f\"FR (Baseline):  {fr_baseline}\")\n",
    "    print(f\"FR (Embedding): {fr_embedding}\")\n",
    "    print(f\"FR (Seq2Seq):   {fr_seq2seq}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_attention_map(model_wrapper, text):\n",
    "    \"\"\"\n",
    "    Generates an attention heatmap for the Cross-Attention layer of the decoder.\n",
    "    This shows which English input words the model focuses on when generating each French word.\n",
    "    \"\"\"\n",
    "    model = model_wrapper.model\n",
    "    tokenizer = model_wrapper.tokenizer\n",
    "    device = model_wrapper.device\n",
    "    \n",
    "    # 1. Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 2. Generate output with attentions\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_length=50\n",
    "        )\n",
    "    \n",
    "    # 3. Process Tokens\n",
    "    # Get input tokens (English)\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "    \n",
    "    # Get output tokens (French) - skip the first token (usually start-of-sequence)\n",
    "    output_ids = outputs.sequences[0]\n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(output_ids)\n",
    "    \n",
    "    # 4. Extract Attention\n",
    "    # outputs.cross_attentions is a tuple of tuples (one per generation step)\n",
    "    # We want the last layer's attention for each step\n",
    "    # Shape: (num_generated_tokens, num_heads, input_length)\n",
    "    \n",
    "    attention_matrix = []\n",
    "    \n",
    "    for step_attentions in outputs.cross_attentions:\n",
    "        # step_attentions contains attentions for all layers at this step\n",
    "        # Get last layer [-1], squeeze batch dim [0]\n",
    "        # Shape: (num_heads, 1, input_seq_len) -> Average over heads\n",
    "        last_layer_attn = step_attentions[-1][0] \n",
    "        avg_attn = last_layer_attn.mean(dim=0).cpu().numpy() # Average over heads\n",
    "        attention_matrix.append(avg_attn)\n",
    "    \n",
    "    # Convert to numpy array (Output Length x Input Length)\n",
    "    attention_matrix = np.array(attention_matrix)\n",
    "    \n",
    "    # Remove padding tokens from visualization if desired\n",
    "    # (Simple slicing based on token lists)\n",
    "    \n",
    "    # 5. Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        attention_matrix,\n",
    "        xticklabels=input_tokens,\n",
    "        yticklabels=output_tokens[1:], # Offset by 1 because cross_attentions starts at 1st generated token\n",
    "        cmap=\"Viridis\",\n",
    "        cbar=True\n",
    "    )\n",
    "    plt.title(f\"Attention Map: '{text}'\")\n",
    "    plt.xlabel(\"Source (English)\")\n",
    "    plt.ylabel(\"Generated Target (French)\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "# Test on a specific example\n",
    "test_sentence = \"The European Parliament must vote on this law.\"\n",
    "print(f\"Visualizing attention for: {test_sentence}\")\n",
    "plot_attention_map(seq2seq_model, test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented and compared two translation approaches:\n",
    "\n",
    "1. **Baseline (Word-for-Word)**: Simple dictionary-based translation that maps each English word to its most common French translation from the training data.\n",
    "\n",
    "2. **Advanced (Cross-Lingual Embeddings)**: Uses multilingual sentence embeddings to find the most semantically similar French sentence from the training corpus.\n",
    "\n",
    "The cross-lingual embedding approach should generally perform better as it considers semantic meaning rather than just word-level mappings. However, it requires more computational resources and may be slower for large candidate pools.\n",
    "\n",
    "### Future Improvements:\n",
    "- Use more sophisticated word alignment algorithms (e.g., IBM models)\n",
    "- Implement phrase-based translation\n",
    "- Use neural sequence-to-sequence models\n",
    "- Fine-tune the embedding model on the specific domain\n",
    "- Add more evaluation metrics (METEOR, ROUGE, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
