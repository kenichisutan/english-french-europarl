{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation: English-French Europarl\n",
    "\n",
    "This notebook implements three translation approaches:\n",
    "1. **Baseline**: Word-for-word translation using a bilingual dictionary\n",
    "2. **Advanced**: Cross-lingual embeddings for semantic translation\n",
    "3. **Best Model**: Fine-tuned Seq2Seq Transformer (iterative improvement)\n",
    "\n",
    "We'll evaluate all models on a train/test split of the Europarl corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library found!\n",
      "accelerate library found! (version 1.12.0)\n",
      "sentencepiece library found!\n",
      "Using device: mps (Apple Silicon GPU)\n",
      "Metal Performance Shaders (MPS) backend enabled\n",
      "Note: Some operations may fall back to CPU if not supported by MPS\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For evaluation\n",
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except ImportError:\n",
    "    print(\"Installing nltk...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'nltk'])\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# For cross-lingual embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    print(\"Installing sentence-transformers...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'sentence-transformers'])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For seq2seq transformer model\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, \n",
    "        AutoModelForSeq2SeqLM, \n",
    "        Seq2SeqTrainingArguments, \n",
    "        Seq2SeqTrainer,\n",
    "        DataCollatorForSeq2Seq\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "    import torch\n",
    "    print(\"Transformers library found!\")\n",
    "except ImportError:\n",
    "    print(\"Installing transformers and datasets...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers', 'datasets', 'accelerate'])\n",
    "    from transformers import (\n",
    "        AutoTokenizer, \n",
    "        AutoModelForSeq2SeqLM, \n",
    "        Seq2SeqTrainingArguments, \n",
    "        Seq2SeqTrainer,\n",
    "        DataCollatorForSeq2Seq\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "    import torch\n",
    "\n",
    "# Check and install accelerate if needed (required for Trainer)\n",
    "try:\n",
    "    import accelerate\n",
    "    # Check version\n",
    "    from packaging import version\n",
    "    if version.parse(accelerate.__version__) < version.parse('0.26.0'):\n",
    "        raise ImportError(\"accelerate version too old\")\n",
    "    print(f\"accelerate library found! (version {accelerate.__version__})\")\n",
    "except (ImportError, AttributeError):\n",
    "    print(\"Installing/upgrading accelerate (required for Trainer)...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'accelerate>=0.26.0'])\n",
    "    import accelerate\n",
    "    print(f\"accelerate installed successfully! (version {accelerate.__version__})\")\n",
    "except Exception as e:\n",
    "    # If packaging not available, try to install anyway\n",
    "    print(\"Checking accelerate version...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'accelerate>=0.26.0'])\n",
    "    import accelerate\n",
    "    print(\"accelerate installed/upgraded!\")\n",
    "\n",
    "# Install sentencepiece if not available (required for some tokenizers like opus-mt)\n",
    "try:\n",
    "    import sentencepiece\n",
    "    print(\"sentencepiece library found!\")\n",
    "except ImportError:\n",
    "    print(\"Installing sentencepiece (required for opus-mt tokenizer)...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'sentencepiece'])\n",
    "    import sentencepiece\n",
    "    print(\"sentencepiece installed successfully!\")\n",
    "\n",
    "# Check for GPU (supports CUDA, MPS for Apple Silicon, and CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Apple Silicon GPU (M1/M2/M3)\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "    print(\"Metal Performance Shaders (MPS) backend enabled\")\n",
    "    print(\"Note: Some operations may fall back to CPU if not supported by MPS\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU)\")\n",
    "    if hasattr(torch.backends, 'mps'):\n",
    "        print(\"MPS not available. Make sure you have PyTorch with MPS support for Apple Silicon.\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/en-fr.tmx...\n",
      "Loaded 100,000 sentence pairs\n",
      "After filtering empty pairs: 100,000 pairs\n"
     ]
    }
   ],
   "source": [
    "def _get_seg_text(elem):\n",
    "    \"\"\"Extract segment text from a tuv element.\"\"\"\n",
    "    seg = elem.find(\".//{*}seg\") or elem.find(\"seg\")\n",
    "    if seg is not None:\n",
    "        return ((seg.text or \"\") + \"\".join((e.tail or \"\") for e in seg)).strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def load_tmx_subset_robust(path, max_tu=None):\n",
    "    \"\"\"\n",
    "    Stream-parse TMX; robust to namespaces. Yields (en_text, fr_text) pairs.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    in_tu = False\n",
    "    en_text = None\n",
    "    fr_text = None\n",
    "\n",
    "    for event, elem in ET.iterparse(path, events=(\"start\", \"end\")):\n",
    "        tag = elem.tag.split(\"}\")[-1] if \"}\" in elem.tag else elem.tag\n",
    "\n",
    "        if tag == \"tu\":\n",
    "            if event == \"start\":\n",
    "                in_tu = True\n",
    "                en_text = None\n",
    "                fr_text = None\n",
    "            else:\n",
    "                if en_text is not None and fr_text is not None:\n",
    "                    pairs.append((en_text, fr_text))\n",
    "                    if max_tu and len(pairs) >= max_tu:\n",
    "                        break\n",
    "                in_tu = False\n",
    "                elem.clear()\n",
    "\n",
    "        elif in_tu and tag == \"tuv\":\n",
    "            if event == \"end\":\n",
    "                lang = elem.get(\n",
    "                    \"{http://www.w3.org/XML/1998/namespace}lang\",\n",
    "                    elem.get(\"lang\", \"\"),\n",
    "                )\n",
    "                text = _get_seg_text(elem)\n",
    "                if lang == \"en\":\n",
    "                    en_text = text\n",
    "                elif lang == \"fr\":\n",
    "                    fr_text = text\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Load data\n",
    "DATA_DIR = Path(\"data\")\n",
    "TMX_PATH = DATA_DIR / \"en-fr.tmx\"\n",
    "\n",
    "print(f\"Loading data from {TMX_PATH}...\")\n",
    "# Load a subset for faster processing (adjust max_tu as needed)\n",
    "# For full dataset, set max_tu=None\n",
    "pairs = load_tmx_subset_robust(TMX_PATH, max_tu=100000)\n",
    "print(f\"Loaded {len(pairs):,} sentence pairs\")\n",
    "\n",
    "# Filter out empty pairs\n",
    "pairs = [(en, fr) for en, fr in pairs if en.strip() and fr.strip()]\n",
    "print(f\"After filtering empty pairs: {len(pairs):,} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 80,000 pairs\n",
      "Test set: 20,000 pairs\n",
      "\n",
      "Sample training pairs:\n",
      "\n",
      "1. EN: We are, of course, in the middle of yet another chaotic summer for flights....\n",
      "   FR: Nous nous trouvons au beau milieu d'un autre été chaotique dans les airs....\n",
      "\n",
      "2. EN: So now we are going to adopt regulations to prevent an epidemic that we have known about since 1986,...\n",
      "   FR: Nous allons donc adopter des règles pour prévenir une épidémie connue depuis 1986 : c'est-à-dire dep...\n",
      "\n",
      "3. EN: I must say to those people who said that employment in the sector would be reduced, that this has no...\n",
      "   FR: Je dois dire à ceux qui disaient que l'emploi allait diminuer dans ce secteur qu'il n'en a pas été a...\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "train_pairs, test_pairs = train_test_split(\n",
    "    pairs, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_pairs):,} pairs\")\n",
    "print(f\"Test set: {len(test_pairs):,} pairs\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample training pairs:\")\n",
    "for i, (en, fr) in enumerate(train_pairs[:3]):\n",
    "    print(f\"\\n{i+1}. EN: {en[:100]}...\")\n",
    "    print(f\"   FR: {fr[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: Word-for-Word Translation\n",
    "\n",
    "This baseline model builds a bilingual dictionary from the training data and translates each word independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bilingual dictionary...\n",
      "Dictionary built with 25,336 English words\n"
     ]
    }
   ],
   "source": [
    "class WordForWordTranslator:\n",
    "    \"\"\"Baseline word-for-word translation using bilingual dictionary.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_dict = defaultdict(lambda: defaultdict(int))\n",
    "        self.most_common_translations = {}\n",
    "        \n",
    "    def train(self, train_pairs):\n",
    "        \"\"\"Build bilingual dictionary from training pairs.\"\"\"\n",
    "        print(\"Building bilingual dictionary...\")\n",
    "        \n",
    "        for en_text, fr_text in train_pairs:\n",
    "            # Simple tokenization (split on whitespace and punctuation)\n",
    "            en_words = re.findall(r'\\b\\w+\\b', en_text.lower())\n",
    "            fr_words = re.findall(r'\\b\\w+\\b', fr_text.lower())\n",
    "            \n",
    "            if not en_words or not fr_words:\n",
    "                continue\n",
    "            \n",
    "            # Use positional alignment based on relative position\n",
    "            # This prevents common words like \"de\" from dominating\n",
    "            en_len = len(en_words)\n",
    "            fr_len = len(fr_words)\n",
    "            \n",
    "            # Align words based on their relative positions\n",
    "            for i, en_word in enumerate(en_words):\n",
    "                # Map English position to French position\n",
    "                fr_pos = int((i / en_len) * fr_len)\n",
    "                fr_pos = min(fr_pos, fr_len - 1)  # Ensure valid index\n",
    "                \n",
    "                # Align to the word at the corresponding position\n",
    "                fr_word = fr_words[fr_pos]\n",
    "                self.word_dict[en_word][fr_word] += 1\n",
    "                \n",
    "                # Also align to nearby words (within 1 position) for better coverage\n",
    "                if fr_pos > 0:\n",
    "                    self.word_dict[en_word][fr_words[fr_pos - 1]] += 0.5\n",
    "                if fr_pos < fr_len - 1:\n",
    "                    self.word_dict[en_word][fr_words[fr_pos + 1]] += 0.5\n",
    "        \n",
    "        # Store most common translation for each English word\n",
    "        for en_word, fr_translations in self.word_dict.items():\n",
    "            if fr_translations:\n",
    "                self.most_common_translations[en_word] = max(\n",
    "                    fr_translations.items(), \n",
    "                    key=lambda x: x[1]\n",
    "                )[0]\n",
    "        \n",
    "        print(f\"Dictionary built with {len(self.most_common_translations):,} English words\")\n",
    "        \n",
    "    def translate(self, en_text):\n",
    "        \"\"\"Translate English text word-by-word.\"\"\"\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en_text)\n",
    "        fr_words = []\n",
    "        \n",
    "        for word in en_words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in self.most_common_translations:\n",
    "                fr_words.append(self.most_common_translations[word_lower])\n",
    "            else:\n",
    "                # Unknown word - keep original\n",
    "                fr_words.append(word)\n",
    "        \n",
    "        return ' '.join(fr_words)\n",
    "    \n",
    "    def translate_preserve_case(self, en_text):\n",
    "        \"\"\"Translate preserving original word casing.\"\"\"\n",
    "        en_words = re.findall(r'\\b\\w+\\b', en_text)\n",
    "        fr_words = []\n",
    "        \n",
    "        for word in en_words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in self.most_common_translations:\n",
    "                translation = self.most_common_translations[word_lower]\n",
    "                # Preserve case\n",
    "                if word[0].isupper():\n",
    "                    translation = translation.capitalize()\n",
    "                fr_words.append(translation)\n",
    "            else:\n",
    "                fr_words.append(word)\n",
    "        \n",
    "        return ' '.join(fr_words)\n",
    "\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = WordForWordTranslator()\n",
    "baseline_model.train(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Translation Examples:\n",
      "\n",
      "Example 1:\n",
      "  EN: The implications are significant for the agro-food sector overall, which has been struck squarely by this crisis.\n",
      "  FR (true):  Les conséquences sont importantes pour l'ensemble de ce secteur agro-alimentaire qui a été touché de plein fouet par cette crise.\n",
      "  FR (pred):  La implications nous un pour la l la secteur de qui a été de elle par ce crise\n",
      "\n",
      "Example 2:\n",
      "  EN: The EUR 614 million adopted by the Council will be enough to finance all the foreseeable needs.\n",
      "  FR (true):  Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "  FR (pred):  La De 1999 millions la par la Conseil de être pas de des tous la à de\n",
      "\n",
      "Example 3:\n",
      "  EN: Joint motion for a resolution (B5-0181/2000) on the shipwreck of the .\n",
      "  FR (true):  Proposition de résolution commune (B5-0181/2000) sur le naufrage de l'Erika.\n",
      "  FR (pred):  De de pour un résolution B5 0181 2000 sur la naufrage de la\n",
      "\n",
      "Example 4:\n",
      "  EN: Following the debates with the Council last night I was wondering whether this report is actually close enough to the original proposal for first reading procedure to be possible.\n",
      "  FR (true):  À la suite aux débats du Conseil hier soir, je me demandais si ce rapport était en fait assez proche de la proposition originale pour permettre la procédure de première lecture.\n",
      "  FR (pred):  La la débats avec la Conseil de de Je a de si ce rapport est en de pas de la la proposition pour première lecture de de être possible\n",
      "\n",
      "Example 5:\n",
      "  EN: Mr Prodi has spoken in favour of preserving the Community model, which met with great approval throughout this Chamber, and I would like, in my turn, to welcome this speech which will doubtless continue to be viewed as a major one.\n",
      "  FR (true):  M. Prodi, président de la Commission, a fait un plaidoyer pour la préservation du modèle communautaire, qui a recueilli un très fort assentiment sur tous ces bancs et je veux, à mon tour, saluer ce discours qui restera, sans nul doute, un discours important.\n",
      "  FR (pred):  Monsieur Prodi a parlé dans en de préservation la La modèle qui et avec de l de ce Cette et Je je voudrais dans mon à de je ce de qui de sans à de être être comme un de un\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the baseline model on a few examples\n",
    "print(\"Baseline Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = baseline_model.translate_preserve_case(en)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Model: Cross-Lingual Embeddings\n",
    "\n",
    "This model uses pre-trained multilingual embeddings to find the best translation by comparing semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cross-lingual embedding model...\n",
      "Loading multilingual embedding model: paraphrase-multilingual-MiniLM-L12-v2...\n",
      "Building French candidate pool...\n",
      "Computing embeddings for 50,000 French candidates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f073c9492ce94ad186af907c081224f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "class CrossLingualEmbeddingTranslator:\n",
    "    \"\"\"Translation using cross-lingual embeddings and semantic similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with a multilingual sentence transformer.\n",
    "        Options:\n",
    "        - 'paraphrase-multilingual-MiniLM-L12-v2' (fast, good quality)\n",
    "        - 'paraphrase-multilingual-mpnet-base-v2' (slower, better quality)\n",
    "        - 'distiluse-base-multilingual-cased' (alternative)\n",
    "        \"\"\"\n",
    "        print(f\"Loading multilingual embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.french_candidates = []\n",
    "        self.french_embeddings = None\n",
    "        \n",
    "    def train(self, train_pairs):\n",
    "        \"\"\"Build a candidate pool of French translations from training data.\"\"\"\n",
    "        print(\"Building French candidate pool...\")\n",
    "        \n",
    "        # Collect unique French sentences (or a large sample)\n",
    "        french_sentences = set()\n",
    "        for _, fr_text in train_pairs:\n",
    "            if fr_text.strip():\n",
    "                french_sentences.add(fr_text.strip())\n",
    "        \n",
    "        # Limit to reasonable size for efficiency (can be adjusted)\n",
    "        max_candidates = 50000\n",
    "        if len(french_sentences) > max_candidates:\n",
    "            french_sentences = list(french_sentences)[:max_candidates]\n",
    "        else:\n",
    "            french_sentences = list(french_sentences)\n",
    "        \n",
    "        self.french_candidates = french_sentences\n",
    "        \n",
    "        # Pre-compute embeddings for all French candidates\n",
    "        print(f\"Computing embeddings for {len(self.french_candidates):,} French candidates...\")\n",
    "        self.french_embeddings = self.model.encode(\n",
    "            self.french_candidates,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "    def translate(self, en_text, top_k=1):\n",
    "        \"\"\"\n",
    "        Translate by finding the most semantically similar French sentence.\n",
    "        \n",
    "        Args:\n",
    "            en_text: English text to translate\n",
    "            top_k: Number of top candidates to return (default: 1, returns best match)\n",
    "        \"\"\"\n",
    "        if not en_text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Get embedding for English text\n",
    "        en_embedding = self.model.encode([en_text])\n",
    "        \n",
    "        # Compute cosine similarity with all French candidates\n",
    "        similarities = cosine_similarity(en_embedding, self.french_embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        if top_k == 1:\n",
    "            return self.french_candidates[top_indices[0]]\n",
    "        else:\n",
    "            return [self.french_candidates[idx] for idx in top_indices]\n",
    "\n",
    "\n",
    "# Train the cross-lingual embedding model\n",
    "print(\"Training cross-lingual embedding model...\")\n",
    "embedding_model = CrossLingualEmbeddingTranslator()\n",
    "embedding_model.train(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Lingual Embedding Translation Examples:\n",
      "\n",
      "Example 1:\n",
      "  EN: The implications are significant for the agro-food sector overall, which has been struck squarely by this crisis.\n",
      "  FR (true):  Les conséquences sont importantes pour l'ensemble de ce secteur agro-alimentaire qui a été touché de plein fouet par cette crise.\n",
      "  FR (pred):  Ce sont justement les bénéfices qui sont au départ des crises actuelles au niveau de la sécurité alimentaire.\n",
      "\n",
      "Example 2:\n",
      "  EN: The EUR 614 million adopted by the Council will be enough to finance all the foreseeable needs.\n",
      "  FR (true):  Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "  FR (pred):  Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "\n",
      "Example 3:\n",
      "  EN: Joint motion for a resolution (B5-0181/2000) on the shipwreck of the .\n",
      "  FR (true):  Proposition de résolution commune (B5-0181/2000) sur le naufrage de l'Erika.\n",
      "  FR (pred):  Résolution commune sur le naufrage de l'Erika (RC B5-0181/2000)\n",
      "\n",
      "Example 4:\n",
      "  EN: Following the debates with the Council last night I was wondering whether this report is actually close enough to the original proposal for first reading procedure to be possible.\n",
      "  FR (true):  À la suite aux débats du Conseil hier soir, je me demandais si ce rapport était en fait assez proche de la proposition originale pour permettre la procédure de première lecture.\n",
      "  FR (pred):  Lorsque j'ai rédigé ce rapport, j'y ai initialement introduit quelques idées polémiques afin de vérifier si on avait pris la peine de le lire.\n",
      "\n",
      "Example 5:\n",
      "  EN: Mr Prodi has spoken in favour of preserving the Community model, which met with great approval throughout this Chamber, and I would like, in my turn, to welcome this speech which will doubtless continue to be viewed as a major one.\n",
      "  FR (true):  M. Prodi, président de la Commission, a fait un plaidoyer pour la préservation du modèle communautaire, qui a recueilli un très fort assentiment sur tous ces bancs et je veux, à mon tour, saluer ce discours qui restera, sans nul doute, un discours important.\n",
      "  FR (pred):  Je conclurai au nom de mon groupe en disant que M. Prodi a exposé aujourd'hui ce en quoi nous croyons.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the embedding model on a few examples\n",
    "print(\"Cross-Lingual Embedding Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = embedding_model.translate(en)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Model: Fine-tuned Seq2Seq Transformer\n",
    "\n",
    "This is our iterative improvement model. We'll fine-tune a pretrained transformer model on our Europarl data to achieve the best translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTranslator:\n",
    "    \"\"\"Fine-tuned Seq2Seq Transformer for English-French translation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='Helsinki-NLP/opus-mt-en-fr', max_length=128):\n",
    "        \"\"\"\n",
    "        Initialize with a pretrained translation model.\n",
    "        \n",
    "        Options:\n",
    "        - 'Helsinki-NLP/opus-mt-en-fr' (fast, good baseline, ~300MB)\n",
    "        - 'facebook/mbart-large-50' (multilingual, better quality, ~2.5GB, needs more GPU)\n",
    "        - 'google/mt5-base' (multilingual T5, good quality, ~850MB)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure sentencepiece is installed (required for opus-mt and some other models)\n",
    "        try:\n",
    "            import sentencepiece\n",
    "        except ImportError:\n",
    "            print(\"Installing sentencepiece (required for this tokenizer)...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sentencepiece'])\n",
    "            import sentencepiece\n",
    "            print(\"sentencepiece installed successfully!\")\n",
    "        \n",
    "        print(f\"Initializing model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "        \n",
    "    def train(self, train_pairs, val_pairs=None, \n",
    "              num_epochs=3, batch_size=8, learning_rate=5e-5,\n",
    "              save_steps=1000, eval_steps=500, warmup_steps=500):\n",
    "        \"\"\"\n",
    "        Fine-tune the model on training data.\n",
    "        \n",
    "        Args:\n",
    "            train_pairs: List of (en, fr) tuples for training\n",
    "            val_pairs: Optional validation set (if None, uses 10% of train)\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Training batch size (adjust based on GPU memory)\n",
    "            learning_rate: Learning rate for fine-tuning\n",
    "            save_steps: Save checkpoint every N steps\n",
    "            eval_steps: Evaluate every N steps\n",
    "            warmup_steps: Warmup steps for learning rate scheduler\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Training Seq2Seq Transformer Model\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Training examples: {len(train_pairs):,}\")\n",
    "        \n",
    "        # Prepare validation set\n",
    "        if val_pairs is None:\n",
    "            # Use 10% of training data for validation\n",
    "            val_size = max(1000, len(train_pairs) // 10)\n",
    "            val_pairs = train_pairs[:val_size]\n",
    "            train_pairs = train_pairs[val_size:]\n",
    "            print(f\"Split: {len(train_pairs):,} train, {len(val_pairs):,} validation\")\n",
    "        \n",
    "        # Convert to HuggingFace Dataset format\n",
    "        def prepare_dataset(pairs):\n",
    "            return Dataset.from_dict({\n",
    "                'en': [pair[0] for pair in pairs],\n",
    "                'fr': [pair[1] for pair in pairs]\n",
    "            })\n",
    "        \n",
    "        train_dataset = prepare_dataset(train_pairs)\n",
    "        val_dataset = prepare_dataset(val_pairs)\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        def tokenize_function(examples):\n",
    "            # Tokenize English (source) and French (target)\n",
    "            model_inputs = self.tokenizer(\n",
    "                examples['en'],\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            \n",
    "            # Tokenize French targets\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                labels = self.tokenizer(\n",
    "                    examples['fr'],\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding='max_length'\n",
    "                )\n",
    "            \n",
    "            # Replace padding token id's of the labels with -100 (ignored by loss)\n",
    "            labels['input_ids'] = [\n",
    "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels['input_ids']\n",
    "            ]\n",
    "            \n",
    "            model_inputs['labels'] = labels['input_ids']\n",
    "            return model_inputs\n",
    "        \n",
    "        print(\"Tokenizing datasets...\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        val_dataset = val_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        output_dir = f\"./seq2seq_model_{self.model_name.split('/')[-1]}\"\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f'{output_dir}/logs',\n",
    "            logging_steps=100,\n",
    "            eval_steps=eval_steps,\n",
    "            save_steps=save_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            fp16=torch.cuda.is_available(),  # Use mixed precision if CUDA GPU available (MPS doesn't support fp16)\n",
    "            report_to=\"none\",  # Disable wandb/tensorboard\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Train!\n",
    "        print(\"\\nStarting training...\")\n",
    "        print(f\"Epochs: {num_epochs}, Batch size: {batch_size}, Learning rate: {learning_rate}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: CUDA available\")\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            print(f\"GPU: Apple Silicon (MPS) available\")\n",
    "        else:\n",
    "            print(f\"Using: CPU\")\n",
    "        \n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def translate(self, en_text, max_length=None, num_beams=4, do_sample=False):\n",
    "        \"\"\"\n",
    "        Translate English text to French.\n",
    "        \n",
    "        Args:\n",
    "            en_text: English text to translate\n",
    "            max_length: Maximum output length (default: self.max_length)\n",
    "            num_beams: Number of beams for beam search (higher = better quality, slower)\n",
    "            do_sample: Whether to use sampling (False = deterministic)\n",
    "        \"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            en_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=do_sample,\n",
    "                early_stopping=True,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation\n",
    "    \n",
    "    def translate_batch(self, en_texts, batch_size=8, **kwargs):\n",
    "        \"\"\"Translate a batch of English texts.\"\"\"\n",
    "        translations = []\n",
    "        for i in range(0, len(en_texts), batch_size):\n",
    "            batch = en_texts[i:i+batch_size]\n",
    "            batch_translations = [self.translate(text, **kwargs) for text in batch]\n",
    "            translations.extend(batch_translations)\n",
    "        return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: Helsinki-NLP/opus-mt-en-fr...\n",
      "Model loaded on mps\n",
      "\n",
      "Model initialized and ready for training!\n",
      "Note: Training will take time. Adjust batch_size and num_epochs based on your GPU memory.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the seq2seq model\n",
    "# You can adjust the model_name to try different pretrained models\n",
    "# For faster training/testing, start with opus-mt-en-fr\n",
    "# For better quality (if you have GPU memory), try mBART or mT5\n",
    "\n",
    "seq2seq_model = Seq2SeqTranslator(\n",
    "    model_name='Helsinki-NLP/opus-mt-en-fr',  # Good baseline, fast\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(\"\\nModel initialized and ready for training!\")\n",
    "print(\"Note: Training will take time. Adjust batch_size and num_epochs based on your GPU memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "**Adjust these parameters based on your resources:**\n",
    "\n",
    "- **CUDA GPU (NVIDIA)**: Use larger batch_size (16-32), more epochs (3-5), fp16 enabled\n",
    "- **Apple Silicon GPU (M1/M2/M3)**: Use moderate batch_size (8-16), more epochs (3-5), no fp16\n",
    "- **CPU only**: Use smaller batch_size (2-4), fewer epochs (1-2), expect slower training\n",
    "- **Limited GPU memory**: Reduce batch_size, use gradient accumulation\n",
    "\n",
    "**Model options:**\n",
    "- `Helsinki-NLP/opus-mt-en-fr`: Fast, ~300MB, good for quick iteration\n",
    "- `facebook/mbart-large-50`: Better quality, ~2.5GB, needs more GPU memory\n",
    "- `google/mt5-base`: Good balance, ~850MB\n",
    "\n",
    "**Note for Apple Silicon (MacBook):**\n",
    "- The code automatically detects and uses MPS (Metal Performance Shaders)\n",
    "- Training will be faster than CPU but may be slower than high-end NVIDIA GPUs\n",
    "- Some operations may fall back to CPU if not supported by MPS\n",
    "- Mixed precision (fp16) is not supported on MPS, so training uses full precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Seq2Seq Transformer Model\n",
      "============================================================\n",
      "Training examples: 80,000\n",
      "Split: 72,000 train, 8,000 validation\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23bc0632dce42c58c52df6d5c65bb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1748454241d443f49ec4bc60c4b72158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epochs: 3, Batch size: 8, Learning rate: 5e-05\n",
      "GPU: Apple Silicon (MPS) available\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27000' max='27000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27000/27000 2:56:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.218300</td>\n",
       "      <td>1.173248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.285200</td>\n",
       "      <td>1.190946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.257800</td>\n",
       "      <td>1.188723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.308700</td>\n",
       "      <td>1.184073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.266800</td>\n",
       "      <td>1.183677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.295100</td>\n",
       "      <td>1.182387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.244700</td>\n",
       "      <td>1.180292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.245600</td>\n",
       "      <td>1.178450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.259900</td>\n",
       "      <td>1.176777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.245000</td>\n",
       "      <td>1.172938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.269900</td>\n",
       "      <td>1.168889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.231200</td>\n",
       "      <td>1.165986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.221800</td>\n",
       "      <td>1.163944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>1.162602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.204800</td>\n",
       "      <td>1.161006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.214600</td>\n",
       "      <td>1.154776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.228300</td>\n",
       "      <td>1.154295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.188800</td>\n",
       "      <td>1.151486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>1.161055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.029200</td>\n",
       "      <td>1.157775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.026700</td>\n",
       "      <td>1.159834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.030600</td>\n",
       "      <td>1.154015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>1.157537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.056800</td>\n",
       "      <td>1.154649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.059000</td>\n",
       "      <td>1.153106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.063000</td>\n",
       "      <td>1.151992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.065300</td>\n",
       "      <td>1.149729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>1.152294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.074200</td>\n",
       "      <td>1.147496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.061500</td>\n",
       "      <td>1.147137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.035600</td>\n",
       "      <td>1.145854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.049100</td>\n",
       "      <td>1.143907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.053800</td>\n",
       "      <td>1.143697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.036400</td>\n",
       "      <td>1.140928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.045300</td>\n",
       "      <td>1.139670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.042600</td>\n",
       "      <td>1.138802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.909800</td>\n",
       "      <td>1.148104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.947300</td>\n",
       "      <td>1.144742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>1.145998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.916500</td>\n",
       "      <td>1.146490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>1.145870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>1.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>1.144098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>1.144819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>1.142187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>1.141725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.897600</td>\n",
       "      <td>1.141006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>1.140615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>1.139028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>1.137742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>1.137122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>1.136064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.914200</td>\n",
       "      <td>1.136246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>1.135735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 1.0713\n",
      "Model saved to ./seq2seq_model_opus-mt-en-fr\n",
      "\n",
      "✅ Training complete! Model is ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Train the seq2seq model\n",
    "# Adjust parameters based on your GPU/CPU and time constraints\n",
    "\n",
    "# For quick testing (CPU or limited time):\n",
    "# seq2seq_model.train(\n",
    "#     train_pairs,\n",
    "#     num_epochs=1,\n",
    "#     batch_size=4 if not torch.cuda.is_available() else 8,\n",
    "#     learning_rate=5e-5,\n",
    "#     save_steps=500,\n",
    "#     eval_steps=250\n",
    "# )\n",
    "\n",
    "# For better results (with GPU):\n",
    "# Determine batch size based on available device\n",
    "if torch.cuda.is_available():\n",
    "    batch_size = 16  # CUDA GPU - can use larger batches\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    batch_size = 8   # Apple Silicon GPU - moderate batch size\n",
    "else:\n",
    "    batch_size = 4   # CPU - smaller batches\n",
    "\n",
    "seq2seq_model.train(\n",
    "    train_pairs,\n",
    "    num_epochs=3,  # Increase to 5-10 for better results\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=5e-5,  # Try 3e-5 or 1e-4 for experimentation\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    warmup_steps=500\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete! Model is ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq Transformer Translation Examples:\n",
      "\n",
      "Example 1:\n",
      "  EN: The implications are significant for the agro-food sector overall, which has been struck squarely by this crisis.\n",
      "  FR (true):  Les conséquences sont importantes pour l'ensemble de ce secteur agro-alimentaire qui a été touché de plein fouet par cette crise.\n",
      "  FR (pred):  Les implications sont importantes pour l'ensemble du secteur agro-alimentaire, qui a été frappé de manière directe par cette crise.\n",
      "\n",
      "Example 2:\n",
      "  EN: The EUR 614 million adopted by the Council will be enough to finance all the foreseeable needs.\n",
      "  FR (true):  Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "  FR (pred):  Les 614 millions d'euros adoptés par le Conseil suffiront à financer l'ensemble des besoins prévisibles.\n",
      "\n",
      "Example 3:\n",
      "  EN: Joint motion for a resolution (B5-0181/2000) on the shipwreck of the .\n",
      "  FR (true):  Proposition de résolution commune (B5-0181/2000) sur le naufrage de l'Erika.\n",
      "  FR (pred):  Proposition de résolution commune (B5-0181/2000) sur le naufrage du navire.\n",
      "\n",
      "Example 4:\n",
      "  EN: Following the debates with the Council last night I was wondering whether this report is actually close enough to the original proposal for first reading procedure to be possible.\n",
      "  FR (true):  À la suite aux débats du Conseil hier soir, je me demandais si ce rapport était en fait assez proche de la proposition originale pour permettre la procédure de première lecture.\n",
      "  FR (pred):  À la suite des débats qui ont eu lieu hier soir avec le Conseil, je me demandais si ce rapport était suffisamment proche de la proposition initiale de procédure de première lecture pour être possible.\n",
      "\n",
      "Example 5:\n",
      "  EN: Mr Prodi has spoken in favour of preserving the Community model, which met with great approval throughout this Chamber, and I would like, in my turn, to welcome this speech which will doubtless continue to be viewed as a major one.\n",
      "  FR (true):  M. Prodi, président de la Commission, a fait un plaidoyer pour la préservation du modèle communautaire, qui a recueilli un très fort assentiment sur tous ces bancs et je veux, à mon tour, saluer ce discours qui restera, sans nul doute, un discours important.\n",
      "  FR (pred):  M. Prodi s'est prononcé en faveur de la préservation du modèle communautaire, qui a reçu une grande approbation dans tout cet hémicycle, et je voudrais à mon tour saluer cette intervention qui restera sans aucun doute importante.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the seq2seq model on a few examples\n",
    "print(\"Seq2Seq Transformer Translation Examples:\\n\")\n",
    "for i, (en, fr_true) in enumerate(test_pairs[:5]):\n",
    "    fr_pred = seq2seq_model.translate(en, num_beams=4)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  EN: {en}\")\n",
    "    print(f\"  FR (true):  {fr_true}\")\n",
    "    print(f\"  FR (pred):  {fr_pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "We'll evaluate all three models using BLEU score and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translations(model, test_pairs, model_name=\"Model\", max_samples=None):\n",
    "    \"\"\"Evaluate translation model on test set.\"\"\"\n",
    "    if max_samples:\n",
    "        test_subset = test_pairs[:max_samples]\n",
    "    else:\n",
    "        test_subset = test_pairs\n",
    "    \n",
    "    print(f\"Evaluating {model_name} on {len(test_subset):,} test examples...\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for i, (en, fr_true) in enumerate(test_subset):\n",
    "        try:\n",
    "            fr_pred = model.translate(en)\n",
    "            predictions.append(fr_pred)\n",
    "            references.append(fr_true)\n",
    "            \n",
    "            # Tokenize for BLEU\n",
    "            pred_tokens = word_tokenize(fr_pred.lower())\n",
    "            ref_tokens = word_tokenize(fr_true.lower())\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            # Exact match\n",
    "            if fr_pred.lower().strip() == fr_true.lower().strip():\n",
    "                exact_matches += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error on example {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i+1:,} examples...\")\n",
    "    \n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match_rate = exact_matches / len(test_subset) if test_subset else 0.0\n",
    "    \n",
    "    results = {\n",
    "        'avg_bleu': avg_bleu,\n",
    "        'exact_match_rate': exact_match_rate,\n",
    "        'num_evaluated': len(test_subset),\n",
    "        'predictions': predictions,\n",
    "        'references': references\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\"  Exact Match Rate: {exact_match_rate:.4f} ({exact_matches}/{len(test_subset)})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluating Baseline (Word-for-Word) on 1,000 test examples...\n",
      "  Processed 1,000 examples...\n",
      "\n",
      "Baseline (Word-for-Word) Results:\n",
      "  Average BLEU Score: 0.0369\n",
      "  Exact Match Rate: 0.0000 (0/1000)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model\n",
    "# Using a subset for faster evaluation (adjust as needed)\n",
    "print(\"=\" * 60)\n",
    "baseline_results = evaluate_translations(\n",
    "    baseline_model, \n",
    "    test_pairs, \n",
    "    model_name=\"Baseline (Word-for-Word)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluating Advanced (Cross-Lingual Embeddings) on 1,000 test examples...\n",
      "  Processed 1,000 examples...\n",
      "\n",
      "Advanced (Cross-Lingual Embeddings) Results:\n",
      "  Average BLEU Score: 0.0387\n",
      "  Exact Match Rate: 0.0020 (2/1000)\n",
      "============================================================\n",
      "Evaluating Best Model (Seq2Seq Transformer) on 1,000 test examples...\n",
      "  Processed 1,000 examples...\n",
      "\n",
      "Best Model (Seq2Seq Transformer) Results:\n",
      "  Average BLEU Score: 0.3136\n",
      "  Exact Match Rate: 0.0220 (22/1000)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate embedding model\n",
    "print(\"=\" * 60)\n",
    "embedding_results = evaluate_translations(\n",
    "    embedding_model,\n",
    "    test_pairs,\n",
    "    model_name=\"Advanced (Cross-Lingual Embeddings)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")\n",
    "\n",
    "# Evaluate seq2seq model\n",
    "print(\"=\" * 60)\n",
    "seq2seq_results = evaluate_translations(\n",
    "    seq2seq_model,\n",
    "    test_pairs,\n",
    "    model_name=\"Best Model (Seq2Seq Transformer)\",\n",
    "    max_samples=1000  # Evaluate on first 1000 for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON - ALL THREE MODELS\n",
      "================================================================================\n",
      "Metric                         Baseline             Embeddings           Seq2Seq             \n",
      "--------------------------------------------------------------------------------\n",
      "Average BLEU Score             0.0369               0.0387               0.3136              \n",
      "Exact Match Rate               0.0000               0.0020               0.0220              \n",
      "Number Evaluated               1000                 1000                 1000                \n",
      "\n",
      "Improvements over Baseline:   \n",
      "  Embeddings: +0.0018 (+4.79%)\n",
      "  Seq2Seq:    +0.2766 (+748.79%)\n",
      "\n",
      "Seq2Seq vs Embeddings: +0.2749 (+710.00%)\n",
      "\n",
      "🏆 Best Model: Seq2Seq (BLEU: 0.3136)\n"
     ]
    }
   ],
   "source": [
    "# Compare all three models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON - ALL THREE MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<30} {'Baseline':<20} {'Embeddings':<20} {'Seq2Seq':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Average BLEU Score':<30} {baseline_results['avg_bleu']:<20.4f} {embedding_results['avg_bleu']:<20.4f} {seq2seq_results['avg_bleu']:<20.4f}\")\n",
    "print(f\"{'Exact Match Rate':<30} {baseline_results['exact_match_rate']:<20.4f} {embedding_results['exact_match_rate']:<20.4f} {seq2seq_results['exact_match_rate']:<20.4f}\")\n",
    "print(f\"{'Number Evaluated':<30} {baseline_results['num_evaluated']:<20} {embedding_results['num_evaluated']:<20} {seq2seq_results['num_evaluated']:<20}\")\n",
    "\n",
    "# Calculate improvements\n",
    "embedding_improvement = embedding_results['avg_bleu'] - baseline_results['avg_bleu']\n",
    "seq2seq_improvement = seq2seq_results['avg_bleu'] - baseline_results['avg_bleu']\n",
    "seq2seq_vs_embedding = seq2seq_results['avg_bleu'] - embedding_results['avg_bleu']\n",
    "\n",
    "print(f\"\\n{'Improvements over Baseline:':<30}\")\n",
    "print(f\"  Embeddings: {embedding_improvement:+.4f} ({embedding_improvement/baseline_results['avg_bleu']*100:+.2f}%)\")\n",
    "print(f\"  Seq2Seq:    {seq2seq_improvement:+.4f} ({seq2seq_improvement/baseline_results['avg_bleu']*100:+.2f}%)\")\n",
    "print(f\"\\nSeq2Seq vs Embeddings: {seq2seq_vs_embedding:+.4f} ({seq2seq_vs_embedding/embedding_results['avg_bleu']*100:+.2f}%)\")\n",
    "\n",
    "# Determine winner\n",
    "best_model = max([\n",
    "    ('Baseline', baseline_results['avg_bleu']),\n",
    "    ('Embeddings', embedding_results['avg_bleu']),\n",
    "    ('Seq2Seq', seq2seq_results['avg_bleu'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model[0]} (BLEU: {best_model[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Examples and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side-by-Side Translation Comparison (All Three Models):\n",
      "\n",
      "================================================================================\n",
      "Example 1\n",
      "================================================================================\n",
      "EN:  The implications are significant for the agro-food sector overall, which has been struck squarely by this crisis.\n",
      "\n",
      "FR (True):      Les conséquences sont importantes pour l'ensemble de ce secteur agro-alimentaire qui a été touché de plein fouet par cette crise.\n",
      "FR (Baseline):  La implications nous un pour la l la secteur de qui a été de elle par ce crise\n",
      "FR (Embedding): Ce sont justement les bénéfices qui sont au départ des crises actuelles au niveau de la sécurité alimentaire.\n",
      "FR (Seq2Seq):   Les implications sont importantes pour l'ensemble du secteur agro-alimentaire, qui a été frappé de manière directe par cette crise.\n",
      "\n",
      "================================================================================\n",
      "Example 2\n",
      "================================================================================\n",
      "EN:  The EUR 614 million adopted by the Council will be enough to finance all the foreseeable needs.\n",
      "\n",
      "FR (True):      Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "FR (Baseline):  La De 1999 millions la par la Conseil de être pas de des tous la à de\n",
      "FR (Embedding): Les 614 millions d'euros retenus par le Conseil permettront de financer l'ensemble des besoins prévisibles.\n",
      "FR (Seq2Seq):   Les 614 millions d'euros adoptés par le Conseil suffiront à financer l'ensemble des besoins prévisibles.\n",
      "\n",
      "================================================================================\n",
      "Example 3\n",
      "================================================================================\n",
      "EN:  Joint motion for a resolution (B5-0181/2000) on the shipwreck of the .\n",
      "\n",
      "FR (True):      Proposition de résolution commune (B5-0181/2000) sur le naufrage de l'Erika.\n",
      "FR (Baseline):  De de pour un résolution B5 0181 2000 sur la naufrage de la\n",
      "FR (Embedding): Résolution commune sur le naufrage de l'Erika (RC B5-0181/2000)\n",
      "FR (Seq2Seq):   Proposition de résolution commune (B5-0181/2000) sur le naufrage du navire.\n",
      "\n",
      "================================================================================\n",
      "Example 4\n",
      "================================================================================\n",
      "EN:  Following the debates with the Council last night I was wondering whether this report is actually close enough to the original proposal for first reading procedure to be possible.\n",
      "\n",
      "FR (True):      À la suite aux débats du Conseil hier soir, je me demandais si ce rapport était en fait assez proche de la proposition originale pour permettre la procédure de première lecture.\n",
      "FR (Baseline):  La la débats avec la Conseil de de Je a de si ce rapport est en de pas de la la proposition pour première lecture de de être possible\n",
      "FR (Embedding): Lorsque j'ai rédigé ce rapport, j'y ai initialement introduit quelques idées polémiques afin de vérifier si on avait pris la peine de le lire.\n",
      "FR (Seq2Seq):   À la suite des débats qui ont eu lieu hier soir avec le Conseil, je me demandais si ce rapport était suffisamment proche de la proposition initiale de procédure de première lecture pour être possible.\n",
      "\n",
      "================================================================================\n",
      "Example 5\n",
      "================================================================================\n",
      "EN:  Mr Prodi has spoken in favour of preserving the Community model, which met with great approval throughout this Chamber, and I would like, in my turn, to welcome this speech which will doubtless continue to be viewed as a major one.\n",
      "\n",
      "FR (True):      M. Prodi, président de la Commission, a fait un plaidoyer pour la préservation du modèle communautaire, qui a recueilli un très fort assentiment sur tous ces bancs et je veux, à mon tour, saluer ce discours qui restera, sans nul doute, un discours important.\n",
      "FR (Baseline):  Monsieur Prodi a parlé dans en de préservation la La modèle qui et avec de l de ce Cette et Je je voudrais dans mon à de je ce de qui de sans à de être être comme un de un\n",
      "FR (Embedding): Je conclurai au nom de mon groupe en disant que M. Prodi a exposé aujourd'hui ce en quoi nous croyons.\n",
      "FR (Seq2Seq):   M. Prodi s'est prononcé en faveur de la préservation du modèle communautaire, qui a reçu une grande approbation dans tout cet hémicycle, et je voudrais à mon tour saluer cette intervention qui restera sans aucun doute importante.\n",
      "\n",
      "================================================================================\n",
      "Example 6\n",
      "================================================================================\n",
      "EN:  My visit to Iraq made me even more intolerant of the fact that, in the name of the defence of democracy and the rights of the Iraqi people, oppressed by Saddam Hussein, we are killing a people and destroying a country.\n",
      "\n",
      "FR (True):      Le voyage que j'ai fait en Irak a rendu encore plus inacceptable à mes yeux le fait qu'au nom de la défense de la démocratie et des droits de la population irakienne, opprimée par Saddam Hussein, on tue un peuple et on détruise un pays.\n",
      "FR (Baseline):  Mon visite de Irak de moi même plus et de la que que dans la nom de la de de démocratie et la droits de la Irakien de souffrances par Saddam Saddam nous nous de un de et des un pays\n",
      "FR (Embedding): Comme les autres orateurs, le sort de la population irakienne me préoccupe.\n",
      "FR (Seq2Seq):   Ma visite en Irak m'a rendu encore plus intolérant devant le fait que, au nom de la défense de la démocratie et des droits du peuple irakien, opprimé par Saddam Hussein, nous tuons un peuple et détruisons un pays.\n",
      "\n",
      "================================================================================\n",
      "Example 7\n",
      "================================================================================\n",
      "EN:  Parliament and the Council voted in favour of a text for the Directive on the patenting of biotechnological discoveries that completely rules out cloning of human beings, because this type of technology offends common decency and is an affront to law and order.\n",
      "\n",
      "FR (True):      Dans le contexte de la directive relative au brevetage des découvertes biotechnologiques, le Parlement et le Conseil ont approuvé un texte qui prévoit l'exclusion complète du clonage d'êtres humains, attendu que cette technique va à l'encontre des bonnes murs et de l'ordre public.\n",
      "FR (Baseline):  Parlement et la Conseil voté dans en de un texte pour la Directive sur la la de inventions le que à règles de clonage de de êtres parce ce de de de est la cohérence et est un un de droit et pour\n",
      "FR (Embedding): Cette affaire nous montre combien il était important, lors des discussions sur la directive relative à la protection juridique des inventions biotechnologiques, que le Parlement ait clairement ancré dans cette directive que les techniques de clonage de l' être humain, les techniques d' intervention sur la ligne germinale et, surtout, sur le corps humain dans toutes les phases de son développement sont exclues de la brevetabilité.\n",
      "FR (Seq2Seq):   Le Parlement et le Conseil ont voté en faveur d'un texte de la directive sur le brevetage des découvertes biotechnologiques qui exclut totalement le clonage des êtres humains, parce que ce type de technologie offense la décence commune et constitue un affront à l'ordre public.\n",
      "\n",
      "================================================================================\n",
      "Example 8\n",
      "================================================================================\n",
      "EN:  Let me repeat to the honourable Member that I have available a table which, in effect, shows that what I have said includes the disparities.\n",
      "\n",
      "FR (True):      Je répète à l'honorable parlementaire que je tiens à disposition un tableau qui montre effectivement que ce que j'ai dit recouvre des disparités.\n",
      "FR (Baseline):  Je moi je de la l États que Je nous de un la qui dans effet montre que ce Je nous dit de la les\n",
      "FR (Embedding): Je suis favorable à cette table ronde.\n",
      "FR (Seq2Seq):   Permettez-moi de répéter à l'honorable parlementaire que j'ai un tableau qui montre effectivement que ce que j'ai dit inclut les différences.\n",
      "\n",
      "================================================================================\n",
      "Example 9\n",
      "================================================================================\n",
      "EN:  I do not congratulate the rapporteur, Mrs Lalumiere;\n",
      "\n",
      "FR (True):      Pour ma part, je ne félicite pas le rapporteur Mme Lalumière.\n",
      "FR (Baseline):  Je ne pas féliciter la rapporteur Mme Lalumiere\n",
      "FR (Embedding): Comme cela n' a pas été le cas, je ne peux dès lors pas appuyer, en toute conscience, la position du rapporteur.\n",
      "FR (Seq2Seq):   Je ne félicite pas le rapporteur, Mme Lalumiere ;\n",
      "\n",
      "================================================================================\n",
      "Example 10\n",
      "================================================================================\n",
      "EN:  But whether or not that will be possible will only become apparent in the autumn of this year once we have the up-to-date progress reports on preparations for accession for each individual candidate country.\n",
      "\n",
      "FR (True):      Mais on ne saura si c'est possible que lorsqu'on disposera, à l'automne de cette année, du rapport actualisé sur les progrès des préparatifs en vue de l'adhésion pour chacun des pays candidats.\n",
      "FR (Baseline):  Mais si ou pas que de être possible de ne de que dans la automne de ce année une nous nous la de de à progrès rapports sur la pour adhésion pour chaque de pays pays\n",
      "FR (Embedding): Enfin, étant donné que je manque de chiffres concrets concernant la situation dans les pays candidats à l'adhésion, je voudrais vous demander s'il est possible d'effectuer des études sur le statu quo dans ces pays candidats et ce, le plus rapidement possible, afin de pouvoir déterminer si des améliorations interviennent.\n",
      "FR (Seq2Seq):   Mais la question de savoir si cela sera possible ne sera évidente qu'à l'automne de cette année, lorsque nous disposerons des rapports d'étape à jour sur la préparation de l'adhésion de chaque pays candidat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show side-by-side comparisons for all three models\n",
    "print(\"Side-by-Side Translation Comparison (All Three Models):\\n\")\n",
    "num_examples = 10\n",
    "for i in range(min(num_examples, len(test_pairs))):\n",
    "    en, fr_true = test_pairs[i]\n",
    "    fr_baseline = baseline_model.translate_preserve_case(en)\n",
    "    fr_embedding = embedding_model.translate(en)\n",
    "    fr_seq2seq = seq2seq_model.translate(en, num_beams=4)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EN:  {en}\")\n",
    "    print(f\"\\nFR (True):      {fr_true}\")\n",
    "    print(f\"FR (Baseline):  {fr_baseline}\")\n",
    "    print(f\"FR (Embedding): {fr_embedding}\")\n",
    "    print(f\"FR (Seq2Seq):   {fr_seq2seq}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented and compared two translation approaches:\n",
    "\n",
    "1. **Baseline (Word-for-Word)**: Simple dictionary-based translation that maps each English word to its most common French translation from the training data.\n",
    "\n",
    "2. **Advanced (Cross-Lingual Embeddings)**: Uses multilingual sentence embeddings to find the most semantically similar French sentence from the training corpus.\n",
    "\n",
    "The cross-lingual embedding approach should generally perform better as it considers semantic meaning rather than just word-level mappings. However, it requires more computational resources and may be slower for large candidate pools.\n",
    "\n",
    "### Future Improvements:\n",
    "- Use more sophisticated word alignment algorithms (e.g., IBM models)\n",
    "- Implement phrase-based translation\n",
    "- Use neural sequence-to-sequence models\n",
    "- Fine-tune the embedding model on the specific domain\n",
    "- Add more evaluation metrics (METEOR, ROUGE, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
